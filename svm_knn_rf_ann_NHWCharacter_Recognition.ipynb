{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the Classification Report\n",
    "target_names = ['क','ख','ग','घ','ङ','च','छ','ज','झ','ञ','ट','ठ',\n",
    "                'ड','ढ','ण','त','थ','द','ध','न','प','फ','ब','भ',\n",
    "                'म','य','र','ल','व','श','ष','स','ह','क्ष','त्र','ज्ञ',\n",
    "                '०','१','२','३','४','५','६','७','८','९']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape:  (78200, 1025)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1*1</th>\n",
       "      <th>1*2</th>\n",
       "      <th>1*3</th>\n",
       "      <th>1*4</th>\n",
       "      <th>1*5</th>\n",
       "      <th>1*6</th>\n",
       "      <th>1*7</th>\n",
       "      <th>1*8</th>\n",
       "      <th>1*9</th>\n",
       "      <th>1*10</th>\n",
       "      <th>...</th>\n",
       "      <th>32*24</th>\n",
       "      <th>32*25</th>\n",
       "      <th>32*26</th>\n",
       "      <th>32*27</th>\n",
       "      <th>32*28</th>\n",
       "      <th>32*29</th>\n",
       "      <th>32*30</th>\n",
       "      <th>32*31</th>\n",
       "      <th>32*32</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1*1  1*2  1*3  1*4  1*5  1*6  1*7  1*8  1*9  1*10  ...  32*24  32*25  \\\n",
       "0    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "1    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "2    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "3    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "4    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "\n",
       "   32*26  32*27  32*28  32*29  32*30  32*31  32*32  Label  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Load train data set\n",
    "data=pd.read_csv(\"D:/Dataset/NHWCSVDataset/nhwDatsetTrain.csv\")\n",
    "print(\"Data Shape: \",data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78200 entries, 0 to 78199\n",
      "Columns: 1025 entries, 1*1 to Label\n",
      "dtypes: int64(1025)\n",
      "memory usage: 611.5 MB\n"
     ]
    }
   ],
   "source": [
    "# data info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique label column\n",
    "np.unique(data['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45    1700\n",
       "11    1700\n",
       "20    1700\n",
       "19    1700\n",
       "18    1700\n",
       "17    1700\n",
       "16    1700\n",
       "15    1700\n",
       "14    1700\n",
       "13    1700\n",
       "12    1700\n",
       "10    1700\n",
       "44    1700\n",
       "9     1700\n",
       "8     1700\n",
       "7     1700\n",
       "6     1700\n",
       "5     1700\n",
       "4     1700\n",
       "3     1700\n",
       "2     1700\n",
       "1     1700\n",
       "21    1700\n",
       "22    1700\n",
       "23    1700\n",
       "24    1700\n",
       "43    1700\n",
       "42    1700\n",
       "41    1700\n",
       "40    1700\n",
       "39    1700\n",
       "38    1700\n",
       "37    1700\n",
       "36    1700\n",
       "35    1700\n",
       "34    1700\n",
       "33    1700\n",
       "32    1700\n",
       "31    1700\n",
       "30    1700\n",
       "29    1700\n",
       "28    1700\n",
       "27    1700\n",
       "26    1700\n",
       "25    1700\n",
       "0     1700\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the total number of each digit count\n",
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'NHW Character')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Characters\n",
    "zero = data.iloc[1, 0:-1]\n",
    "zero = zero.values.reshape(32,32)\n",
    "plt.imshow(zero)\n",
    "plt.title(\"NHW Character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Tin')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.subplot(2,3,1)  # 2--rows and 2--columns  index=1\n",
    "zero = data.iloc[1, 0:-1]\n",
    "zero = zero.values.reshape(32,32)\n",
    "plt.imshow(zero)\n",
    "plt.title(\" Ka \")\n",
    "\n",
    "plt.subplot(2,3,2)  # 2--rows and 2--columns  index=2\n",
    "zero = data.iloc[9001, 0:-1]\n",
    "zero = zero.values.reshape(32,32)\n",
    "plt.imshow(zero)\n",
    "plt.title(\"Cha\")\n",
    "\n",
    "plt.subplot(2,3,3)   # 2--rows and 2--columns  index=3\n",
    "zero = data.iloc[31000, 0:-1]\n",
    "zero = zero.values.reshape(32,32)\n",
    "plt.imshow(zero)\n",
    "plt.title(\"Dha\")\n",
    "\n",
    "plt.subplot(2,3,4)   # 2--rows and 2--columns  index=3\n",
    "zero = data.iloc[53000, 0:-1]\n",
    "zero = zero.values.reshape(32,32)\n",
    "plt.imshow(zero)\n",
    "plt.title(\"Sa\")\n",
    "\n",
    "plt.subplot(2,3,5)   # 2--rows and 2--columns  index=3\n",
    "zero = data.iloc[63000, 0:-1]\n",
    "zero = zero.values.reshape(32,32)\n",
    "plt.imshow(zero)\n",
    "plt.title(\"Eka\")\n",
    "\n",
    "plt.subplot(2,3,6)   # 2--rows and 2--columns  index=3\n",
    "zero = data.iloc[67000, 0:-1]\n",
    "zero = zero.values.reshape(32,32)\n",
    "plt.imshow(zero)\n",
    "plt.title(\"Tin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split Dataset in Features and Traget Variables\n",
    "X=data.iloc[:,0:-1]\n",
    "Y=data.iloc[:,-1]\n",
    "#print(\"X: \",Train_X)\n",
    "#print(\"Y: \",Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 3. If needed , Normalize data\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (58650, 1024)\n",
      "X_test:  (19550, 1024)\n",
      "Y_train:  (58650,)\n",
      "Y_test:  (19550,)\n"
     ]
    }
   ],
   "source": [
    "# 4. Split Dataset into training and Testing sets\n",
    "Train_X,Test_X, Train_Y,Test_Y=train_test_split(X,Y,test_size=0.25,random_state=1)\n",
    "print(\"X_train: \",Train_X.shape)\n",
    "print(\"X_test: \",Test_X.shape)\n",
    "print(\"Y_train: \",Train_Y.shape)\n",
    "print(\"Y_test: \",Test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Define the Classifier Model\n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "# C:Regularization parameter. default=1.0. The strength of the regularization is inversely proportional to C. \n",
    "# degreeint, default=3. Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "# decision_function_shape{‘ovo’, ‘ovr’}, default=’ovr’\n",
    "\n",
    "model=SVC(C=0.5, kernel='rbf',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=1, kernel='rbf',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "#print(confusion_matrix(Test_Y,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=50, kernel='rbf',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=500, kernel='rbf',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=0.1, kernel='linear',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=1, kernel='linear',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=50, kernel='linear',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=500, kernel='linear',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=0.1, kernel='poly',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=1, kernel='poly',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=50, kernel='poly',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC(C=500, kernel='poly',decision_function_shape='ovr')\n",
    "model=model.fit(Train_X,Train_Y)\n",
    "\n",
    "#7. Predict the response for test dataset\n",
    "ypred=model.predict(Test_X)\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# 8. Print The Model Accuracy\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "error_rate = []\n",
    "for i in range(1,9):\n",
    "    j=2*i+1\n",
    "    knn = KNeighborsClassifier(n_neighbors=j,metric='minkowski',p=2)\n",
    "    knn.fit(Train_X,Train_Y)\n",
    "    pred_i = knn.predict(Test_X)\n",
    "    error_rate.append(np.mean(pred_i != Test_Y))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([1,3,5,7,9,11,13,15],error_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "print(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate)))\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=3,metric='minkowski',p=2)\n",
    "knnmodel=knnmodel.fit(Train_X,Train_Y)\n",
    "ypred=knnmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "\n",
    "print(confusion_matrix(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\n",
    "knnmodel=knnmodel.fit(Train_X,Train_Y)\n",
    "ypred=knnmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "\n",
    "print(confusion_matrix(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=7,metric='minkowski',p=2)\n",
    "knnmodel=knnmodel.fit(Train_X,Train_Y)\n",
    "ypred=knnmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "\n",
    "print(confusion_matrix(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=7,metric='minkowski',p=1)\n",
    "knnmodel=knnmodel.fit(Train_X,Train_Y)\n",
    "ypred=knnmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "\n",
    "print(confusion_matrix(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbors algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=1)\n",
    "knnmodel=knnmodel.fit(Train_X,Train_Y)\n",
    "ypred=knnmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "\n",
    "print(confusion_matrix(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8869053708439898\n",
      "[[385   1   1 ...   0   0   0]\n",
      " [  0 370   6 ...   0   0   0]\n",
      " [  0   0 406 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 439   0   0]\n",
      " [  0   0   0 ...   0 431   0]\n",
      " [  0   0   0 ...   0   0 441]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.97      0.93      0.95       414\n",
      "           ख       0.96      0.89      0.92       417\n",
      "           ग       0.75      0.94      0.84       431\n",
      "           घ       0.77      0.85      0.81       448\n",
      "         Nha       0.86      0.83      0.85       422\n",
      "           च       0.86      0.94      0.90       423\n",
      "           छ       0.93      0.88      0.90       445\n",
      "           ज       0.82      0.94      0.88       423\n",
      "           झ       1.00      0.92      0.96       442\n",
      "           ञ       0.90      0.91      0.91       448\n",
      "           ट       0.67      0.94      0.78       420\n",
      "           ठ       0.89      0.80      0.85       463\n",
      "           ड       0.81      0.82      0.82       443\n",
      "           ढ       0.87      0.84      0.86       430\n",
      "           ण       0.89      0.88      0.89       416\n",
      "           त       0.73      0.93      0.82       414\n",
      "           थ       0.88      0.80      0.84       406\n",
      "           द       0.89      0.86      0.88       437\n",
      "           ध       0.91      0.83      0.87       410\n",
      "           न       0.86      0.83      0.84       419\n",
      "           प       0.55      0.91      0.69       370\n",
      "           फ       0.97      0.87      0.92       457\n",
      "           ब       0.94      0.69      0.80       431\n",
      "           भ       0.94      0.87      0.90       398\n",
      "           म       0.93      0.75      0.83       462\n",
      "           य       0.82      0.81      0.82       406\n",
      "           र       0.95      0.91      0.93       425\n",
      "           ल       0.91      0.95      0.93       434\n",
      "           व       0.81      0.79      0.80       442\n",
      "           श       0.95      0.90      0.92       374\n",
      "           ष       0.92      0.73      0.81       415\n",
      "           स       0.94      0.83      0.88       394\n",
      "           ह       0.96      0.83      0.89       462\n",
      "         क्ष       1.00      0.87      0.93       437\n",
      "         त्र       0.94      0.89      0.91       404\n",
      "         ज्ञ       0.96      0.90      0.93       405\n",
      "           ०       0.97      0.99      0.98       407\n",
      "           १       0.94      1.00      0.97       382\n",
      "           २       0.88      0.98      0.92       427\n",
      "           ३       0.98      0.91      0.94       415\n",
      "           ४       0.99      0.97      0.98       414\n",
      "           ५       0.98      0.96      0.97       442\n",
      "           ६       0.96      0.97      0.97       446\n",
      "           ७       0.99      0.99      0.99       443\n",
      "           ८       0.87      0.99      0.93       437\n",
      "           ९       0.98      0.98      0.98       450\n",
      "\n",
      "    accuracy                           0.89     19550\n",
      "   macro avg       0.90      0.89      0.89     19550\n",
      "weighted avg       0.90      0.89      0.89     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nearest Neighbors algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=3,metric='minkowski',p=1)\n",
    "knnmodel=knnmodel.fit(Train_X,Train_Y)\n",
    "ypred=knnmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "\n",
    "print(confusion_matrix(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8706393861892583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.88      0.92      0.90       414\n",
      "           ख       0.85      0.83      0.84       417\n",
      "           ग       0.88      0.87      0.87       431\n",
      "           घ       0.79      0.77      0.78       448\n",
      "         Nha       0.86      0.86      0.86       422\n",
      "           च       0.81      0.90      0.85       423\n",
      "           छ       0.85      0.78      0.81       445\n",
      "           ज       0.90      0.89      0.89       423\n",
      "           झ       0.93      0.92      0.92       442\n",
      "           ञ       0.90      0.89      0.89       448\n",
      "           ट       0.86      0.93      0.90       420\n",
      "           ठ       0.87      0.92      0.89       463\n",
      "           ड       0.85      0.82      0.84       443\n",
      "           ढ       0.87      0.90      0.89       430\n",
      "           ण       0.87      0.86      0.86       416\n",
      "           त       0.87      0.93      0.90       414\n",
      "           थ       0.81      0.75      0.78       406\n",
      "           द       0.84      0.82      0.83       437\n",
      "           ध       0.83      0.80      0.81       410\n",
      "           न       0.82      0.90      0.86       419\n",
      "           प       0.68      0.89      0.77       370\n",
      "           फ       0.91      0.94      0.92       457\n",
      "           ब       0.91      0.75      0.82       431\n",
      "           भ       0.89      0.84      0.87       398\n",
      "           म       0.83      0.74      0.78       462\n",
      "           य       0.78      0.78      0.78       406\n",
      "           र       0.90      0.89      0.90       425\n",
      "           ल       0.91      0.92      0.92       434\n",
      "           व       0.85      0.75      0.80       442\n",
      "           श       0.81      0.85      0.83       374\n",
      "           ष       0.77      0.84      0.80       415\n",
      "           स       0.83      0.75      0.79       394\n",
      "           ह       0.92      0.80      0.85       462\n",
      "         क्ष       0.85      0.83      0.84       437\n",
      "         त्र       0.84      0.86      0.85       404\n",
      "         ज्ञ       0.90      0.88      0.89       405\n",
      "           ०       0.96      0.99      0.97       407\n",
      "           १       0.92      0.99      0.95       382\n",
      "           २       0.87      0.94      0.91       427\n",
      "           ३       0.90      0.88      0.89       415\n",
      "           ४       0.96      0.95      0.96       414\n",
      "           ५       0.95      0.94      0.95       442\n",
      "           ६       0.93      0.93      0.93       446\n",
      "           ७       0.97      0.98      0.98       443\n",
      "           ८       0.94      0.95      0.94       437\n",
      "           ९       0.95      0.95      0.95       450\n",
      "\n",
      "    accuracy                           0.87     19550\n",
      "   macro avg       0.87      0.87      0.87     19550\n",
      "weighted avg       0.87      0.87      0.87     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=51,criterion='gini',max_features='log2')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8782097186700767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.91      0.92      0.91       414\n",
      "           ख       0.84      0.83      0.83       417\n",
      "           ग       0.91      0.85      0.88       431\n",
      "           घ       0.81      0.78      0.80       448\n",
      "           ङ       0.88      0.85      0.87       422\n",
      "           च       0.81      0.90      0.85       423\n",
      "           छ       0.85      0.78      0.81       445\n",
      "           ज       0.89      0.87      0.88       423\n",
      "           झ       0.93      0.91      0.92       442\n",
      "           ञ       0.90      0.90      0.90       448\n",
      "           ट       0.88      0.94      0.91       420\n",
      "           ठ       0.89      0.93      0.91       463\n",
      "           ड       0.87      0.84      0.86       443\n",
      "           ढ       0.87      0.91      0.89       430\n",
      "           ण       0.89      0.89      0.89       416\n",
      "           त       0.92      0.94      0.93       414\n",
      "           थ       0.80      0.79      0.80       406\n",
      "           द       0.88      0.82      0.85       437\n",
      "           ध       0.84      0.81      0.83       410\n",
      "           न       0.82      0.90      0.86       419\n",
      "           प       0.70      0.88      0.78       370\n",
      "           फ       0.90      0.93      0.92       457\n",
      "           ब       0.90      0.78      0.84       431\n",
      "           भ       0.87      0.89      0.88       398\n",
      "           म       0.84      0.77      0.80       462\n",
      "           य       0.79      0.77      0.78       406\n",
      "           र       0.91      0.90      0.91       425\n",
      "           ल       0.90      0.92      0.91       434\n",
      "           व       0.87      0.81      0.84       442\n",
      "           श       0.83      0.87      0.85       374\n",
      "           ष       0.80      0.85      0.83       415\n",
      "           स       0.82      0.77      0.80       394\n",
      "           ह       0.91      0.78      0.84       462\n",
      "         क्ष       0.85      0.83      0.84       437\n",
      "         त्र       0.87      0.88      0.88       404\n",
      "         ज्ञ       0.87      0.90      0.89       405\n",
      "           ०       0.95      0.99      0.97       407\n",
      "           १       0.92      0.98      0.95       382\n",
      "           २       0.89      0.94      0.91       427\n",
      "           ३       0.90      0.87      0.89       415\n",
      "           ४       0.95      0.94      0.95       414\n",
      "           ५       0.94      0.95      0.94       442\n",
      "           ६       0.93      0.92      0.93       446\n",
      "           ७       0.96      0.98      0.97       443\n",
      "           ८       0.94      0.95      0.94       437\n",
      "           ९       0.95      0.97      0.96       450\n",
      "\n",
      "    accuracy                           0.88     19550\n",
      "   macro avg       0.88      0.88      0.88     19550\n",
      "weighted avg       0.88      0.88      0.88     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=51,criterion='gini',max_features='auto')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8759079283887468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.90      0.91      0.90       414\n",
      "           ख       0.85      0.85      0.85       417\n",
      "           ग       0.87      0.87      0.87       431\n",
      "           घ       0.78      0.79      0.78       448\n",
      "           ङ       0.87      0.83      0.85       422\n",
      "           च       0.83      0.91      0.86       423\n",
      "           छ       0.87      0.79      0.83       445\n",
      "           ज       0.91      0.91      0.91       423\n",
      "           झ       0.95      0.92      0.93       442\n",
      "           ञ       0.90      0.87      0.89       448\n",
      "           ट       0.85      0.92      0.89       420\n",
      "           ठ       0.88      0.92      0.90       463\n",
      "           ड       0.83      0.82      0.82       443\n",
      "           ढ       0.89      0.91      0.90       430\n",
      "           ण       0.89      0.89      0.89       416\n",
      "           त       0.89      0.94      0.91       414\n",
      "           थ       0.81      0.81      0.81       406\n",
      "           द       0.86      0.82      0.84       437\n",
      "           ध       0.83      0.78      0.80       410\n",
      "           न       0.82      0.90      0.86       419\n",
      "           प       0.67      0.86      0.75       370\n",
      "           फ       0.89      0.93      0.91       457\n",
      "           ब       0.87      0.77      0.82       431\n",
      "           भ       0.86      0.85      0.86       398\n",
      "           म       0.88      0.75      0.81       462\n",
      "           य       0.79      0.80      0.79       406\n",
      "           र       0.92      0.88      0.90       425\n",
      "           ल       0.89      0.91      0.90       434\n",
      "           व       0.86      0.78      0.81       442\n",
      "           श       0.85      0.85      0.85       374\n",
      "           ष       0.78      0.84      0.81       415\n",
      "           स       0.85      0.77      0.81       394\n",
      "           ह       0.89      0.79      0.84       462\n",
      "         क्ष       0.86      0.84      0.85       437\n",
      "         त्र       0.88      0.88      0.88       404\n",
      "         ज्ञ       0.90      0.89      0.89       405\n",
      "           ०       0.94      0.99      0.96       407\n",
      "           १       0.91      0.99      0.95       382\n",
      "           २       0.86      0.96      0.91       427\n",
      "           ३       0.93      0.89      0.91       415\n",
      "           ४       0.96      0.96      0.96       414\n",
      "           ५       0.96      0.93      0.95       442\n",
      "           ६       0.94      0.95      0.94       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.94      0.96      0.95       437\n",
      "           ९       0.96      0.95      0.96       450\n",
      "\n",
      "    accuracy                           0.88     19550\n",
      "   macro avg       0.88      0.88      0.88     19550\n",
      "weighted avg       0.88      0.88      0.88     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=51,criterion='entropy',max_features='log2')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8859335038363171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.93      0.94      0.93       414\n",
      "           ख       0.87      0.85      0.86       417\n",
      "           ग       0.92      0.88      0.90       431\n",
      "           घ       0.81      0.79      0.80       448\n",
      "           ङ       0.87      0.85      0.86       422\n",
      "           च       0.84      0.92      0.88       423\n",
      "           छ       0.87      0.81      0.84       445\n",
      "           ज       0.90      0.90      0.90       423\n",
      "           झ       0.94      0.91      0.93       442\n",
      "           ञ       0.92      0.89      0.91       448\n",
      "           ट       0.89      0.93      0.91       420\n",
      "           ठ       0.89      0.93      0.91       463\n",
      "           ड       0.86      0.84      0.85       443\n",
      "           ढ       0.89      0.94      0.92       430\n",
      "           ण       0.89      0.92      0.90       416\n",
      "           त       0.90      0.94      0.92       414\n",
      "           थ       0.83      0.81      0.82       406\n",
      "           द       0.85      0.84      0.85       437\n",
      "           ध       0.86      0.82      0.84       410\n",
      "           न       0.82      0.91      0.86       419\n",
      "           प       0.72      0.89      0.79       370\n",
      "           फ       0.91      0.96      0.94       457\n",
      "           ब       0.87      0.76      0.81       431\n",
      "           भ       0.90      0.88      0.89       398\n",
      "           म       0.87      0.78      0.82       462\n",
      "           य       0.80      0.79      0.79       406\n",
      "           र       0.91      0.90      0.91       425\n",
      "           ल       0.90      0.92      0.91       434\n",
      "           व       0.86      0.80      0.83       442\n",
      "           श       0.84      0.89      0.87       374\n",
      "           ष       0.81      0.85      0.83       415\n",
      "           स       0.85      0.76      0.80       394\n",
      "           ह       0.94      0.82      0.87       462\n",
      "         क्ष       0.87      0.85      0.86       437\n",
      "         त्र       0.86      0.89      0.87       404\n",
      "         ज्ञ       0.91      0.90      0.90       405\n",
      "           ०       0.95      0.99      0.97       407\n",
      "           १       0.92      0.99      0.95       382\n",
      "           २       0.88      0.94      0.91       427\n",
      "           ३       0.90      0.89      0.89       415\n",
      "           ४       0.97      0.95      0.96       414\n",
      "           ५       0.96      0.93      0.94       442\n",
      "           ६       0.93      0.94      0.94       446\n",
      "           ७       0.99      0.97      0.98       443\n",
      "           ८       0.94      0.95      0.94       437\n",
      "           ९       0.95      0.96      0.96       450\n",
      "\n",
      "    accuracy                           0.89     19550\n",
      "   macro avg       0.89      0.89      0.89     19550\n",
      "weighted avg       0.89      0.89      0.89     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=51,criterion='entropy',max_features='auto')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8901790281329923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.91      0.92      0.91       414\n",
      "           ख       0.88      0.86      0.87       417\n",
      "           ग       0.91      0.86      0.89       431\n",
      "           घ       0.83      0.79      0.81       448\n",
      "           ङ       0.90      0.89      0.89       422\n",
      "           च       0.84      0.91      0.87       423\n",
      "           छ       0.91      0.80      0.85       445\n",
      "           ज       0.91      0.88      0.90       423\n",
      "           झ       0.95      0.93      0.94       442\n",
      "           ञ       0.92      0.89      0.90       448\n",
      "           ट       0.88      0.94      0.91       420\n",
      "           ठ       0.90      0.93      0.91       463\n",
      "           ड       0.87      0.84      0.85       443\n",
      "           ढ       0.89      0.93      0.91       430\n",
      "           ण       0.90      0.90      0.90       416\n",
      "           त       0.90      0.93      0.91       414\n",
      "           थ       0.86      0.83      0.84       406\n",
      "           द       0.88      0.83      0.85       437\n",
      "           ध       0.85      0.85      0.85       410\n",
      "           न       0.85      0.90      0.88       419\n",
      "           प       0.71      0.90      0.79       370\n",
      "           फ       0.90      0.95      0.92       457\n",
      "           ब       0.92      0.79      0.85       431\n",
      "           भ       0.89      0.87      0.88       398\n",
      "           म       0.88      0.79      0.83       462\n",
      "           य       0.82      0.83      0.82       406\n",
      "           र       0.90      0.91      0.91       425\n",
      "           ल       0.91      0.94      0.92       434\n",
      "           व       0.86      0.79      0.83       442\n",
      "           श       0.88      0.90      0.89       374\n",
      "           ष       0.78      0.86      0.82       415\n",
      "           स       0.87      0.78      0.82       394\n",
      "           ह       0.91      0.81      0.86       462\n",
      "         क्ष       0.88      0.86      0.87       437\n",
      "         त्र       0.86      0.89      0.88       404\n",
      "         ज्ञ       0.88      0.91      0.89       405\n",
      "           ०       0.95      0.99      0.97       407\n",
      "           १       0.90      0.99      0.94       382\n",
      "           २       0.88      0.95      0.92       427\n",
      "           ३       0.92      0.90      0.91       415\n",
      "           ४       0.95      0.96      0.95       414\n",
      "           ५       0.97      0.95      0.96       442\n",
      "           ६       0.94      0.94      0.94       446\n",
      "           ७       0.97      0.98      0.98       443\n",
      "           ८       0.95      0.96      0.95       437\n",
      "           ९       0.96      0.96      0.96       450\n",
      "\n",
      "    accuracy                           0.89     19550\n",
      "   macro avg       0.89      0.89      0.89     19550\n",
      "weighted avg       0.89      0.89      0.89     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=101,criterion='gini',max_features='log2')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8959079283887468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.91      0.92      0.91       414\n",
      "           ख       0.89      0.87      0.88       417\n",
      "           ग       0.91      0.88      0.89       431\n",
      "           घ       0.83      0.80      0.82       448\n",
      "           ङ       0.89      0.89      0.89       422\n",
      "           च       0.84      0.91      0.88       423\n",
      "           छ       0.90      0.80      0.85       445\n",
      "           ज       0.92      0.90      0.91       423\n",
      "           झ       0.94      0.91      0.93       442\n",
      "           ञ       0.93      0.89      0.91       448\n",
      "           ट       0.91      0.93      0.92       420\n",
      "           ठ       0.91      0.94      0.92       463\n",
      "           ड       0.90      0.87      0.89       443\n",
      "           ढ       0.90      0.94      0.92       430\n",
      "           ण       0.90      0.91      0.91       416\n",
      "           त       0.89      0.95      0.92       414\n",
      "           थ       0.85      0.82      0.84       406\n",
      "           द       0.88      0.85      0.86       437\n",
      "           ध       0.89      0.86      0.87       410\n",
      "           न       0.85      0.93      0.89       419\n",
      "           प       0.73      0.92      0.81       370\n",
      "           फ       0.90      0.95      0.93       457\n",
      "           ब       0.91      0.79      0.85       431\n",
      "           भ       0.88      0.88      0.88       398\n",
      "           म       0.88      0.80      0.84       462\n",
      "           य       0.80      0.81      0.81       406\n",
      "           र       0.93      0.92      0.93       425\n",
      "           ल       0.91      0.93      0.92       434\n",
      "           व       0.87      0.84      0.85       442\n",
      "           श       0.87      0.89      0.88       374\n",
      "           ष       0.80      0.87      0.84       415\n",
      "           स       0.86      0.79      0.82       394\n",
      "           ह       0.94      0.84      0.89       462\n",
      "         क्ष       0.89      0.85      0.87       437\n",
      "         त्र       0.88      0.90      0.89       404\n",
      "         ज्ञ       0.89      0.92      0.91       405\n",
      "           ०       0.96      0.99      0.98       407\n",
      "           १       0.92      0.99      0.95       382\n",
      "           २       0.88      0.94      0.91       427\n",
      "           ३       0.91      0.89      0.90       415\n",
      "           ४       0.96      0.95      0.96       414\n",
      "           ५       0.94      0.95      0.94       442\n",
      "           ६       0.94      0.93      0.94       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.95      0.96      0.95       437\n",
      "           ९       0.96      0.98      0.97       450\n",
      "\n",
      "    accuracy                           0.90     19550\n",
      "   macro avg       0.90      0.90      0.90     19550\n",
      "weighted avg       0.90      0.90      0.90     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=101,criterion='gini',max_features='auto')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8936061381074168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.92      0.92      0.92       414\n",
      "           ख       0.91      0.86      0.88       417\n",
      "           ग       0.90      0.87      0.88       431\n",
      "           घ       0.83      0.78      0.80       448\n",
      "           ङ       0.90      0.90      0.90       422\n",
      "           च       0.84      0.91      0.87       423\n",
      "           छ       0.91      0.79      0.85       445\n",
      "           ज       0.93      0.89      0.91       423\n",
      "           झ       0.95      0.93      0.94       442\n",
      "           ञ       0.94      0.89      0.91       448\n",
      "           ट       0.89      0.94      0.92       420\n",
      "           ठ       0.90      0.92      0.91       463\n",
      "           ड       0.88      0.85      0.87       443\n",
      "           ढ       0.88      0.92      0.90       430\n",
      "           ण       0.90      0.91      0.91       416\n",
      "           त       0.90      0.95      0.93       414\n",
      "           थ       0.86      0.81      0.83       406\n",
      "           द       0.88      0.85      0.86       437\n",
      "           ध       0.84      0.84      0.84       410\n",
      "           न       0.88      0.91      0.89       419\n",
      "           प       0.70      0.90      0.79       370\n",
      "           फ       0.90      0.95      0.93       457\n",
      "           ब       0.91      0.77      0.84       431\n",
      "           भ       0.89      0.88      0.88       398\n",
      "           म       0.88      0.78      0.83       462\n",
      "           य       0.80      0.80      0.80       406\n",
      "           र       0.92      0.92      0.92       425\n",
      "           ल       0.90      0.94      0.92       434\n",
      "           व       0.87      0.83      0.85       442\n",
      "           श       0.86      0.90      0.88       374\n",
      "           ष       0.81      0.88      0.84       415\n",
      "           स       0.85      0.80      0.83       394\n",
      "           ह       0.91      0.82      0.87       462\n",
      "         क्ष       0.89      0.88      0.88       437\n",
      "         त्र       0.88      0.91      0.90       404\n",
      "         ज्ञ       0.90      0.91      0.90       405\n",
      "           ०       0.95      0.99      0.97       407\n",
      "           १       0.92      0.99      0.95       382\n",
      "           २       0.88      0.95      0.91       427\n",
      "           ३       0.93      0.89      0.91       415\n",
      "           ४       0.97      0.96      0.96       414\n",
      "           ५       0.97      0.96      0.96       442\n",
      "           ६       0.93      0.95      0.94       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.94      0.95      0.95       437\n",
      "           ९       0.96      0.96      0.96       450\n",
      "\n",
      "    accuracy                           0.89     19550\n",
      "   macro avg       0.89      0.89      0.89     19550\n",
      "weighted avg       0.89      0.89      0.89     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=101,criterion='entropy',max_features='log2')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.89923273657289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.92      0.93      0.93       414\n",
      "           ख       0.90      0.87      0.88       417\n",
      "           ग       0.93      0.89      0.91       431\n",
      "           घ       0.86      0.81      0.84       448\n",
      "           ङ       0.91      0.87      0.89       422\n",
      "           च       0.87      0.92      0.89       423\n",
      "           छ       0.89      0.80      0.84       445\n",
      "           ज       0.93      0.90      0.91       423\n",
      "           झ       0.94      0.93      0.94       442\n",
      "           ञ       0.95      0.90      0.92       448\n",
      "           ट       0.89      0.94      0.91       420\n",
      "           ठ       0.90      0.93      0.92       463\n",
      "           ड       0.90      0.87      0.88       443\n",
      "           ढ       0.90      0.94      0.92       430\n",
      "           ण       0.92      0.93      0.92       416\n",
      "           त       0.91      0.94      0.93       414\n",
      "           थ       0.85      0.85      0.85       406\n",
      "           द       0.88      0.84      0.86       437\n",
      "           ध       0.89      0.85      0.87       410\n",
      "           न       0.85      0.92      0.88       419\n",
      "           प       0.75      0.90      0.82       370\n",
      "           फ       0.93      0.96      0.94       457\n",
      "           ब       0.92      0.80      0.86       431\n",
      "           भ       0.88      0.90      0.89       398\n",
      "           म       0.89      0.80      0.85       462\n",
      "           य       0.81      0.84      0.82       406\n",
      "           र       0.92      0.92      0.92       425\n",
      "           ल       0.91      0.94      0.92       434\n",
      "           व       0.89      0.83      0.86       442\n",
      "           श       0.85      0.90      0.87       374\n",
      "           ष       0.81      0.88      0.84       415\n",
      "           स       0.87      0.81      0.84       394\n",
      "           ह       0.91      0.84      0.87       462\n",
      "         क्ष       0.87      0.87      0.87       437\n",
      "         त्र       0.88      0.92      0.90       404\n",
      "         ज्ञ       0.90      0.91      0.91       405\n",
      "           ०       0.95      0.98      0.97       407\n",
      "           १       0.91      0.98      0.95       382\n",
      "           २       0.89      0.94      0.92       427\n",
      "           ३       0.91      0.88      0.89       415\n",
      "           ४       0.94      0.96      0.95       414\n",
      "           ५       0.96      0.95      0.95       442\n",
      "           ६       0.93      0.95      0.94       446\n",
      "           ७       0.99      0.98      0.98       443\n",
      "           ८       0.95      0.94      0.95       437\n",
      "           ९       0.96      0.96      0.96       450\n",
      "\n",
      "    accuracy                           0.90     19550\n",
      "   macro avg       0.90      0.90      0.90     19550\n",
      "weighted avg       0.90      0.90      0.90     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=101,criterion='entropy',max_features='auto')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9033759590792839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.93      0.93      0.93       414\n",
      "           ख       0.90      0.89      0.89       417\n",
      "           ग       0.93      0.88      0.91       431\n",
      "           घ       0.85      0.82      0.83       448\n",
      "           ङ       0.92      0.88      0.90       422\n",
      "           च       0.85      0.91      0.88       423\n",
      "           छ       0.92      0.79      0.85       445\n",
      "           ज       0.94      0.89      0.91       423\n",
      "           झ       0.94      0.93      0.93       442\n",
      "           ञ       0.95      0.91      0.93       448\n",
      "           ट       0.90      0.95      0.93       420\n",
      "           ठ       0.91      0.94      0.92       463\n",
      "           ड       0.89      0.87      0.88       443\n",
      "           ढ       0.91      0.95      0.93       430\n",
      "           ण       0.91      0.92      0.92       416\n",
      "           त       0.92      0.94      0.93       414\n",
      "           थ       0.86      0.82      0.84       406\n",
      "           द       0.88      0.86      0.87       437\n",
      "           ध       0.88      0.85      0.87       410\n",
      "           न       0.86      0.95      0.90       419\n",
      "           प       0.73      0.91      0.81       370\n",
      "           फ       0.94      0.95      0.95       457\n",
      "           ब       0.91      0.80      0.85       431\n",
      "           भ       0.91      0.89      0.90       398\n",
      "           म       0.89      0.81      0.85       462\n",
      "           य       0.82      0.85      0.83       406\n",
      "           र       0.93      0.92      0.93       425\n",
      "           ल       0.93      0.94      0.93       434\n",
      "           व       0.88      0.83      0.86       442\n",
      "           श       0.86      0.92      0.89       374\n",
      "           ष       0.80      0.88      0.84       415\n",
      "           स       0.88      0.81      0.85       394\n",
      "           ह       0.93      0.85      0.89       462\n",
      "         क्ष       0.87      0.87      0.87       437\n",
      "         त्र       0.90      0.92      0.91       404\n",
      "         ज्ञ       0.91      0.93      0.92       405\n",
      "           ०       0.96      0.99      0.97       407\n",
      "           १       0.92      0.99      0.95       382\n",
      "           २       0.89      0.95      0.92       427\n",
      "           ३       0.91      0.90      0.91       415\n",
      "           ४       0.97      0.96      0.97       414\n",
      "           ५       0.96      0.96      0.96       442\n",
      "           ६       0.94      0.93      0.94       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.94      0.95      0.95       437\n",
      "           ९       0.95      0.97      0.96       450\n",
      "\n",
      "    accuracy                           0.90     19550\n",
      "   macro avg       0.90      0.90      0.90     19550\n",
      "weighted avg       0.90      0.90      0.90     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=151,criterion='gini',max_features='auto')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8982608695652174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.91      0.92      0.92       414\n",
      "           ख       0.93      0.88      0.91       417\n",
      "           ग       0.92      0.88      0.90       431\n",
      "           घ       0.84      0.78      0.81       448\n",
      "           ङ       0.92      0.88      0.90       422\n",
      "           च       0.85      0.91      0.88       423\n",
      "           छ       0.90      0.80      0.85       445\n",
      "           ज       0.94      0.90      0.92       423\n",
      "           झ       0.96      0.93      0.94       442\n",
      "           ञ       0.94      0.91      0.92       448\n",
      "           ट       0.90      0.94      0.92       420\n",
      "           ठ       0.91      0.94      0.92       463\n",
      "           ड       0.88      0.84      0.86       443\n",
      "           ढ       0.89      0.94      0.91       430\n",
      "           ण       0.92      0.89      0.91       416\n",
      "           त       0.90      0.95      0.92       414\n",
      "           थ       0.85      0.83      0.84       406\n",
      "           द       0.88      0.83      0.86       437\n",
      "           ध       0.86      0.85      0.85       410\n",
      "           न       0.87      0.91      0.89       419\n",
      "           प       0.73      0.89      0.80       370\n",
      "           फ       0.92      0.96      0.94       457\n",
      "           ब       0.93      0.78      0.85       431\n",
      "           भ       0.90      0.90      0.90       398\n",
      "           म       0.89      0.80      0.84       462\n",
      "           य       0.82      0.86      0.84       406\n",
      "           र       0.93      0.93      0.93       425\n",
      "           ल       0.91      0.93      0.92       434\n",
      "           व       0.88      0.83      0.85       442\n",
      "           श       0.85      0.91      0.88       374\n",
      "           ष       0.79      0.88      0.83       415\n",
      "           स       0.85      0.79      0.82       394\n",
      "           ह       0.91      0.83      0.87       462\n",
      "         क्ष       0.86      0.88      0.87       437\n",
      "         त्र       0.89      0.92      0.90       404\n",
      "         ज्ञ       0.90      0.91      0.90       405\n",
      "           ०       0.96      0.99      0.97       407\n",
      "           १       0.90      0.98      0.94       382\n",
      "           २       0.87      0.94      0.91       427\n",
      "           ३       0.92      0.90      0.91       415\n",
      "           ४       0.97      0.97      0.97       414\n",
      "           ५       0.97      0.95      0.96       442\n",
      "           ६       0.94      0.96      0.95       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.94      0.97      0.95       437\n",
      "           ९       0.95      0.97      0.96       450\n",
      "\n",
      "    accuracy                           0.90     19550\n",
      "   macro avg       0.90      0.90      0.90     19550\n",
      "weighted avg       0.90      0.90      0.90     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=151,criterion='gini',max_features='log2')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9061381074168798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.94      0.94      0.94       414\n",
      "           ख       0.90      0.88      0.89       417\n",
      "           ग       0.94      0.89      0.91       431\n",
      "           घ       0.86      0.81      0.83       448\n",
      "           ङ       0.91      0.87      0.89       422\n",
      "           च       0.86      0.92      0.89       423\n",
      "           छ       0.93      0.81      0.87       445\n",
      "           ज       0.92      0.90      0.91       423\n",
      "           झ       0.96      0.93      0.95       442\n",
      "           ञ       0.94      0.89      0.92       448\n",
      "           ट       0.89      0.93      0.91       420\n",
      "           ठ       0.90      0.95      0.92       463\n",
      "           ड       0.89      0.87      0.88       443\n",
      "           ढ       0.90      0.94      0.92       430\n",
      "           ण       0.91      0.93      0.92       416\n",
      "           त       0.92      0.95      0.93       414\n",
      "           थ       0.85      0.84      0.84       406\n",
      "           द       0.88      0.84      0.86       437\n",
      "           ध       0.89      0.86      0.87       410\n",
      "           न       0.85      0.93      0.89       419\n",
      "           प       0.73      0.92      0.81       370\n",
      "           फ       0.94      0.95      0.94       457\n",
      "           ब       0.91      0.81      0.86       431\n",
      "           भ       0.92      0.91      0.91       398\n",
      "           म       0.91      0.85      0.88       462\n",
      "           य       0.82      0.83      0.82       406\n",
      "           र       0.94      0.92      0.93       425\n",
      "           ल       0.92      0.94      0.93       434\n",
      "           व       0.89      0.84      0.86       442\n",
      "           श       0.87      0.90      0.89       374\n",
      "           ष       0.82      0.90      0.86       415\n",
      "           स       0.92      0.83      0.87       394\n",
      "           ह       0.91      0.86      0.88       462\n",
      "         क्ष       0.89      0.88      0.89       437\n",
      "         त्र       0.89      0.92      0.91       404\n",
      "         ज्ञ       0.90      0.93      0.91       405\n",
      "           ०       0.97      0.99      0.98       407\n",
      "           १       0.93      0.99      0.96       382\n",
      "           २       0.91      0.94      0.92       427\n",
      "           ३       0.91      0.92      0.92       415\n",
      "           ४       0.97      0.96      0.96       414\n",
      "           ५       0.97      0.96      0.97       442\n",
      "           ६       0.94      0.96      0.95       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.95      0.96      0.96       437\n",
      "           ९       0.96      0.96      0.96       450\n",
      "\n",
      "    accuracy                           0.91     19550\n",
      "   macro avg       0.91      0.91      0.91     19550\n",
      "weighted avg       0.91      0.91      0.91     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=151,criterion='entropy',max_features='auto')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9039897698209719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.94      0.93      0.94       414\n",
      "           ख       0.91      0.87      0.89       417\n",
      "           ग       0.93      0.88      0.91       431\n",
      "           घ       0.84      0.79      0.82       448\n",
      "           ङ       0.91      0.88      0.90       422\n",
      "           च       0.84      0.91      0.88       423\n",
      "           छ       0.91      0.82      0.86       445\n",
      "           ज       0.93      0.91      0.92       423\n",
      "           झ       0.96      0.93      0.94       442\n",
      "           ञ       0.94      0.90      0.92       448\n",
      "           ट       0.89      0.95      0.92       420\n",
      "           ठ       0.92      0.94      0.93       463\n",
      "           ड       0.89      0.86      0.88       443\n",
      "           ढ       0.91      0.94      0.92       430\n",
      "           ण       0.95      0.92      0.93       416\n",
      "           त       0.91      0.95      0.93       414\n",
      "           थ       0.88      0.81      0.84       406\n",
      "           द       0.89      0.86      0.88       437\n",
      "           ध       0.89      0.88      0.89       410\n",
      "           न       0.84      0.92      0.88       419\n",
      "           प       0.72      0.90      0.80       370\n",
      "           फ       0.92      0.96      0.94       457\n",
      "           ब       0.93      0.82      0.87       431\n",
      "           भ       0.92      0.91      0.91       398\n",
      "           म       0.91      0.81      0.86       462\n",
      "           य       0.81      0.83      0.82       406\n",
      "           र       0.93      0.92      0.93       425\n",
      "           ल       0.91      0.95      0.93       434\n",
      "           व       0.90      0.84      0.87       442\n",
      "           श       0.87      0.93      0.90       374\n",
      "           ष       0.79      0.87      0.83       415\n",
      "           स       0.86      0.81      0.84       394\n",
      "           ह       0.92      0.84      0.88       462\n",
      "         क्ष       0.88      0.88      0.88       437\n",
      "         त्र       0.88      0.92      0.90       404\n",
      "         ज्ञ       0.93      0.93      0.93       405\n",
      "           ०       0.95      0.99      0.97       407\n",
      "           १       0.91      0.99      0.95       382\n",
      "           २       0.89      0.94      0.92       427\n",
      "           ३       0.92      0.90      0.91       415\n",
      "           ४       0.97      0.96      0.96       414\n",
      "           ५       0.98      0.96      0.97       442\n",
      "           ६       0.93      0.95      0.94       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.95      0.97      0.96       437\n",
      "           ९       0.96      0.97      0.96       450\n",
      "\n",
      "    accuracy                           0.90     19550\n",
      "   macro avg       0.90      0.90      0.90     19550\n",
      "weighted avg       0.91      0.90      0.90     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=151,criterion='entropy',max_features='log2')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9081329923273658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           क       0.94      0.93      0.94       414\n",
      "           ख       0.93      0.89      0.91       417\n",
      "           ग       0.95      0.91      0.93       431\n",
      "           घ       0.87      0.82      0.84       448\n",
      "           ङ       0.93      0.87      0.90       422\n",
      "           च       0.86      0.91      0.89       423\n",
      "           छ       0.92      0.82      0.86       445\n",
      "           ज       0.94      0.91      0.92       423\n",
      "           झ       0.96      0.94      0.95       442\n",
      "           ञ       0.95      0.90      0.93       448\n",
      "           ट       0.90      0.94      0.92       420\n",
      "           ठ       0.90      0.94      0.92       463\n",
      "           ड       0.90      0.89      0.90       443\n",
      "           ढ       0.92      0.95      0.93       430\n",
      "           ण       0.93      0.93      0.93       416\n",
      "           त       0.92      0.95      0.93       414\n",
      "           थ       0.88      0.84      0.86       406\n",
      "           द       0.89      0.84      0.86       437\n",
      "           ध       0.88      0.87      0.87       410\n",
      "           न       0.86      0.93      0.89       419\n",
      "           प       0.73      0.91      0.81       370\n",
      "           फ       0.94      0.96      0.95       457\n",
      "           ब       0.91      0.82      0.86       431\n",
      "           भ       0.91      0.90      0.91       398\n",
      "           म       0.91      0.83      0.86       462\n",
      "           य       0.85      0.85      0.85       406\n",
      "           र       0.92      0.93      0.92       425\n",
      "           ल       0.91      0.94      0.93       434\n",
      "           व       0.87      0.85      0.86       442\n",
      "           श       0.88      0.92      0.90       374\n",
      "           ष       0.81      0.90      0.85       415\n",
      "           स       0.87      0.84      0.85       394\n",
      "           ह       0.93      0.85      0.89       462\n",
      "         क्ष       0.87      0.89      0.88       437\n",
      "         त्र       0.88      0.92      0.90       404\n",
      "         ज्ञ       0.93      0.92      0.92       405\n",
      "           ०       0.95      0.99      0.97       407\n",
      "           १       0.92      0.99      0.96       382\n",
      "           २       0.90      0.95      0.92       427\n",
      "           ३       0.93      0.90      0.91       415\n",
      "           ४       0.96      0.96      0.96       414\n",
      "           ५       0.96      0.97      0.96       442\n",
      "           ६       0.93      0.95      0.94       446\n",
      "           ७       0.98      0.98      0.98       443\n",
      "           ८       0.96      0.95      0.96       437\n",
      "           ९       0.96      0.97      0.96       450\n",
      "\n",
      "    accuracy                           0.91     19550\n",
      "   macro avg       0.91      0.91      0.91     19550\n",
      "weighted avg       0.91      0.91      0.91     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forrest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "# If “auto”, then max_features=sqrt(n_features).\n",
    "# If “log2”, then max_features=log2(n_features).\n",
    "# criterion{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "rfmodel= RandomForestClassifier(n_estimators=251,criterion='entropy',max_features='auto')\n",
    "rfmodel.fit(Train_X,Train_Y)\n",
    "ypred=rfmodel.predict(Test_X)\n",
    "print(\"accuracy:\", accuracy_score(Test_Y,ypred))\n",
    "print(classification_report(Test_Y, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape:  (78200, 1025)\n",
      "Test Data Shape:  (13800, 1025)\n"
     ]
    }
   ],
   "source": [
    "#1. Load train data set\n",
    "data=pd.read_csv(\"D:/Dataset/NHWCSVDataset/nhwDatsetTrain.csv\")\n",
    "print(\"Train Data Shape: \",data.shape)\n",
    "#data.head()\n",
    "Train_X=data.iloc[:,0:-1]\n",
    "Train_Y=data.iloc[:,-1]\n",
    "#1. Load test data set\n",
    "data=pd.read_csv(\"D:/Dataset/NHWCSVDataset/nhwDatsetTest.csv\")\n",
    "print(\"Test Data Shape: \",data.shape)\n",
    "#print(data.head())\n",
    "Test_X=data.iloc[:,0:-1]\n",
    "Test_Y=data.iloc[:,-1]\n",
    "#print(\"X: \",Test_X)\n",
    "#print(\"Y: \",Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 3. If needed , Normalize data\n",
    "Train_X = MinMaxScaler().fit_transform(Train_X)\n",
    "Test_X = MinMaxScaler().fit_transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (78200, 1024)\n",
      "X_test:  (13800, 1024)\n",
      "Y_train:  (78200,)\n",
      "Y_test:  (13800,)\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset into training and Testing sets\n",
    "#X_train,X_test, Y_train,Y_test =train_test_split(X,Y,test_size=0.2,random_state=1)\n",
    "print(\"X_train: \",Train_X.shape)\n",
    "print(\"X_test: \",Test_X.shape)\n",
    "print(\"Y_train: \",Train_Y.shape)\n",
    "print(\"Y_test: \",Test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# convert to one-hot vector\n",
    "Train_Y = to_categorical(Train_Y)\n",
    "Test_Y = to_categorical(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Train_Y)\n",
    "#print(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 5.4258 - accuracy: 0.2911 - val_loss: 4.1651 - val_accuracy: 0.5252\n",
      "Epoch 2/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 3.7039 - accuracy: 0.5929 - val_loss: 3.3401 - val_accuracy: 0.6533\n",
      "Epoch 3/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 3.1521 - accuracy: 0.6756 - val_loss: 2.9797 - val_accuracy: 0.6977\n",
      "Epoch 4/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 2.8551 - accuracy: 0.7168 - val_loss: 2.7305 - val_accuracy: 0.7311\n",
      "Epoch 5/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 2.6391 - accuracy: 0.7472 - val_loss: 2.5478 - val_accuracy: 0.7540\n",
      "Epoch 6/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 2.4632 - accuracy: 0.7709 - val_loss: 2.3842 - val_accuracy: 0.7777\n",
      "Epoch 7/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 2.3170 - accuracy: 0.7897 - val_loss: 2.2560 - val_accuracy: 0.7930\n",
      "Epoch 8/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 2.1904 - accuracy: 0.8063 - val_loss: 2.1380 - val_accuracy: 0.8059\n",
      "Epoch 9/300\n",
      "306/306 [==============================] - 4s 15ms/step - loss: 2.0795 - accuracy: 0.8217 - val_loss: 2.0401 - val_accuracy: 0.8180\n",
      "Epoch 10/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.9836 - accuracy: 0.8314 - val_loss: 1.9529 - val_accuracy: 0.8304\n",
      "Epoch 11/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.8979 - accuracy: 0.8427 - val_loss: 1.8795 - val_accuracy: 0.8370\n",
      "Epoch 12/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.8246 - accuracy: 0.8506 - val_loss: 1.8026 - val_accuracy: 0.8482\n",
      "Epoch 13/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.7580 - accuracy: 0.8578 - val_loss: 1.7470 - val_accuracy: 0.8538\n",
      "Epoch 14/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.6979 - accuracy: 0.8656 - val_loss: 1.6952 - val_accuracy: 0.8588\n",
      "Epoch 15/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.6449 - accuracy: 0.8714 - val_loss: 1.6476 - val_accuracy: 0.8641\n",
      "Epoch 16/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.5960 - accuracy: 0.8770 - val_loss: 1.5986 - val_accuracy: 0.8696\n",
      "Epoch 17/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.5525 - accuracy: 0.8811 - val_loss: 1.5564 - val_accuracy: 0.8733\n",
      "Epoch 18/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.5117 - accuracy: 0.8862 - val_loss: 1.5230 - val_accuracy: 0.8757\n",
      "Epoch 19/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.4757 - accuracy: 0.8895 - val_loss: 1.4917 - val_accuracy: 0.8800\n",
      "Epoch 20/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.4408 - accuracy: 0.8934 - val_loss: 1.4594 - val_accuracy: 0.8809\n",
      "Epoch 21/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.4101 - accuracy: 0.8960 - val_loss: 1.4264 - val_accuracy: 0.8845\n",
      "Epoch 22/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.3789 - accuracy: 0.8995 - val_loss: 1.4001 - val_accuracy: 0.8864\n",
      "Epoch 23/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.3532 - accuracy: 0.9016 - val_loss: 1.3787 - val_accuracy: 0.8883\n",
      "Epoch 24/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.3258 - accuracy: 0.9043 - val_loss: 1.3581 - val_accuracy: 0.8893\n",
      "Epoch 25/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.3028 - accuracy: 0.9067 - val_loss: 1.3269 - val_accuracy: 0.8931\n",
      "Epoch 26/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.2786 - accuracy: 0.9089 - val_loss: 1.3165 - val_accuracy: 0.8925\n",
      "Epoch 27/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.2573 - accuracy: 0.9112 - val_loss: 1.2897 - val_accuracy: 0.8952\n",
      "Epoch 28/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.2358 - accuracy: 0.9130 - val_loss: 1.2749 - val_accuracy: 0.8973\n",
      "Epoch 29/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.2162 - accuracy: 0.9157 - val_loss: 1.2527 - val_accuracy: 0.8974\n",
      "Epoch 30/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.1968 - accuracy: 0.9168 - val_loss: 1.2360 - val_accuracy: 0.8985\n",
      "Epoch 31/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.1785 - accuracy: 0.9189 - val_loss: 1.2204 - val_accuracy: 0.9013\n",
      "Epoch 32/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.1612 - accuracy: 0.9205 - val_loss: 1.2074 - val_accuracy: 0.8993\n",
      "Epoch 33/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.1458 - accuracy: 0.9216 - val_loss: 1.1897 - val_accuracy: 0.9029\n",
      "Epoch 34/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.1299 - accuracy: 0.9229 - val_loss: 1.1775 - val_accuracy: 0.9043\n",
      "Epoch 35/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.1142 - accuracy: 0.9238 - val_loss: 1.1581 - val_accuracy: 0.9051\n",
      "Epoch 36/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.0998 - accuracy: 0.9256 - val_loss: 1.1526 - val_accuracy: 0.9059\n",
      "Epoch 37/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.0856 - accuracy: 0.9263 - val_loss: 1.1347 - val_accuracy: 0.9038\n",
      "Epoch 38/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.0716 - accuracy: 0.9279 - val_loss: 1.1261 - val_accuracy: 0.9070\n",
      "Epoch 39/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.0586 - accuracy: 0.9297 - val_loss: 1.1141 - val_accuracy: 0.9094\n",
      "Epoch 40/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.0468 - accuracy: 0.9301 - val_loss: 1.1035 - val_accuracy: 0.9099\n",
      "Epoch 41/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.0341 - accuracy: 0.9312 - val_loss: 1.0934 - val_accuracy: 0.9075\n",
      "Epoch 42/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.0229 - accuracy: 0.9323 - val_loss: 1.0761 - val_accuracy: 0.9121\n",
      "Epoch 43/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 1.0122 - accuracy: 0.9332 - val_loss: 1.0693 - val_accuracy: 0.9094\n",
      "Epoch 44/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 1.0009 - accuracy: 0.9340 - val_loss: 1.0642 - val_accuracy: 0.9084\n",
      "Epoch 45/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.9909 - accuracy: 0.9354 - val_loss: 1.0543 - val_accuracy: 0.9107\n",
      "Epoch 46/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.9818 - accuracy: 0.9371 - val_loss: 1.0423 - val_accuracy: 0.9140\n",
      "Epoch 47/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.9719 - accuracy: 0.9379 - val_loss: 1.0345 - val_accuracy: 0.9132\n",
      "Epoch 48/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.9626 - accuracy: 0.9377 - val_loss: 1.0257 - val_accuracy: 0.9127\n",
      "Epoch 49/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.9552 - accuracy: 0.9385 - val_loss: 1.0166 - val_accuracy: 0.9143\n",
      "Epoch 50/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.9472 - accuracy: 0.9393 - val_loss: 1.0124 - val_accuracy: 0.9140\n",
      "Epoch 51/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.9369 - accuracy: 0.9403 - val_loss: 1.0050 - val_accuracy: 0.9145\n",
      "Epoch 52/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.9294 - accuracy: 0.9409 - val_loss: 1.0010 - val_accuracy: 0.9146\n",
      "Epoch 53/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.9218 - accuracy: 0.9422 - val_loss: 0.9865 - val_accuracy: 0.9151\n",
      "Epoch 54/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.9137 - accuracy: 0.9431 - val_loss: 0.9870 - val_accuracy: 0.9143\n",
      "Epoch 55/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.9063 - accuracy: 0.9436 - val_loss: 0.9769 - val_accuracy: 0.9157\n",
      "Epoch 56/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.8994 - accuracy: 0.9441 - val_loss: 0.9707 - val_accuracy: 0.9177s - loss: 0.8997 - ac\n",
      "Epoch 57/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.8931 - accuracy: 0.9443 - val_loss: 0.9672 - val_accuracy: 0.9145\n",
      "Epoch 58/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.8872 - accuracy: 0.9441 - val_loss: 0.9609 - val_accuracy: 0.9157\n",
      "Epoch 59/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.8800 - accuracy: 0.9457 - val_loss: 0.9539 - val_accuracy: 0.9151\n",
      "Epoch 60/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.8731 - accuracy: 0.9466 - val_loss: 0.9476 - val_accuracy: 0.9178\n",
      "Epoch 61/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.8674 - accuracy: 0.9470 - val_loss: 0.9490 - val_accuracy: 0.9154\n",
      "Epoch 62/300\n",
      "306/306 [==============================] - 5s 16ms/step - loss: 0.8613 - accuracy: 0.9478 - val_loss: 0.9390 - val_accuracy: 0.9195\n",
      "Epoch 63/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.8563 - accuracy: 0.9481 - val_loss: 0.9332 - val_accuracy: 0.9184\n",
      "Epoch 64/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.8501 - accuracy: 0.9488 - val_loss: 0.9324 - val_accuracy: 0.9175\n",
      "Epoch 65/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.8455 - accuracy: 0.9485 - val_loss: 0.9262 - val_accuracy: 0.9175\n",
      "Epoch 66/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.8403 - accuracy: 0.9493 - val_loss: 0.9216 - val_accuracy: 0.9186\n",
      "Epoch 67/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.8334 - accuracy: 0.9507 - val_loss: 0.9232 - val_accuracy: 0.9154\n",
      "Epoch 68/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.8292 - accuracy: 0.9510 - val_loss: 0.9134 - val_accuracy: 0.9189\n",
      "Epoch 69/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.8241 - accuracy: 0.9511 - val_loss: 0.9025 - val_accuracy: 0.9204\n",
      "Epoch 70/300\n",
      "306/306 [==============================] - 4s 15ms/step - loss: 0.8201 - accuracy: 0.9511 - val_loss: 0.9020 - val_accuracy: 0.9207\n",
      "Epoch 71/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.8158 - accuracy: 0.9523 - val_loss: 0.9001 - val_accuracy: 0.9188\n",
      "Epoch 72/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.8115 - accuracy: 0.9525 - val_loss: 0.9030 - val_accuracy: 0.9172\n",
      "Epoch 73/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.8072 - accuracy: 0.9527 - val_loss: 0.8898 - val_accuracy: 0.9210\n",
      "Epoch 74/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.8021 - accuracy: 0.9539 - val_loss: 0.8871 - val_accuracy: 0.9211\n",
      "Epoch 75/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.7991 - accuracy: 0.9537 - val_loss: 0.8865 - val_accuracy: 0.9199\n",
      "Epoch 76/300\n",
      "306/306 [==============================] - 4s 15ms/step - loss: 0.7946 - accuracy: 0.9539 - val_loss: 0.8784 - val_accuracy: 0.9224\n",
      "Epoch 77/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.7908 - accuracy: 0.9551 - val_loss: 0.8735 - val_accuracy: 0.9222\n",
      "Epoch 78/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.7858 - accuracy: 0.9559 - val_loss: 0.8745 - val_accuracy: 0.9233\n",
      "Epoch 79/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7818 - accuracy: 0.9561 - val_loss: 0.8718 - val_accuracy: 0.9213\n",
      "Epoch 80/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7788 - accuracy: 0.9558 - val_loss: 0.8651 - val_accuracy: 0.9244\n",
      "Epoch 81/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7745 - accuracy: 0.9569 - val_loss: 0.8639 - val_accuracy: 0.9222\n",
      "Epoch 82/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7721 - accuracy: 0.9571 - val_loss: 0.8670 - val_accuracy: 0.9206\n",
      "Epoch 83/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7684 - accuracy: 0.9567 - val_loss: 0.8580 - val_accuracy: 0.9224\n",
      "Epoch 84/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7643 - accuracy: 0.9580 - val_loss: 0.8552 - val_accuracy: 0.9207\n",
      "Epoch 85/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7620 - accuracy: 0.9572 - val_loss: 0.8537 - val_accuracy: 0.9220\n",
      "Epoch 86/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7577 - accuracy: 0.9582 - val_loss: 0.8509 - val_accuracy: 0.9231\n",
      "Epoch 87/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.7552 - accuracy: 0.9583 - val_loss: 0.8466 - val_accuracy: 0.9243\n",
      "Epoch 88/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7516 - accuracy: 0.9589 - val_loss: 0.8440 - val_accuracy: 0.9255\n",
      "Epoch 89/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7478 - accuracy: 0.9599 - val_loss: 0.8476 - val_accuracy: 0.9225\n",
      "Epoch 90/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7455 - accuracy: 0.9598 - val_loss: 0.8413 - val_accuracy: 0.9244\n",
      "Epoch 91/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7437 - accuracy: 0.9595 - val_loss: 0.8384 - val_accuracy: 0.9228\n",
      "Epoch 92/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7390 - accuracy: 0.9605 - val_loss: 0.8419 - val_accuracy: 0.9225\n",
      "Epoch 93/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7372 - accuracy: 0.9603 - val_loss: 0.8330 - val_accuracy: 0.9239\n",
      "Epoch 94/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7336 - accuracy: 0.9607 - val_loss: 0.8350 - val_accuracy: 0.9243\n",
      "Epoch 95/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7303 - accuracy: 0.9610 - val_loss: 0.8300 - val_accuracy: 0.9239\n",
      "Epoch 96/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7277 - accuracy: 0.9616 - val_loss: 0.8286 - val_accuracy: 0.9238\n",
      "Epoch 97/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7253 - accuracy: 0.9616 - val_loss: 0.8218 - val_accuracy: 0.9253\n",
      "Epoch 98/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7220 - accuracy: 0.9624 - val_loss: 0.8203 - val_accuracy: 0.9249\n",
      "Epoch 99/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7202 - accuracy: 0.9620 - val_loss: 0.8187 - val_accuracy: 0.9250\n",
      "Epoch 100/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7170 - accuracy: 0.9624 - val_loss: 0.8203 - val_accuracy: 0.9248\n",
      "Epoch 101/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7141 - accuracy: 0.9625 - val_loss: 0.8178 - val_accuracy: 0.9241\n",
      "Epoch 102/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7118 - accuracy: 0.9638 - val_loss: 0.8181 - val_accuracy: 0.9238\n",
      "Epoch 103/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7084 - accuracy: 0.9636 - val_loss: 0.8145 - val_accuracy: 0.9260\n",
      "Epoch 104/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7056 - accuracy: 0.9644 - val_loss: 0.8129 - val_accuracy: 0.9236\n",
      "Epoch 105/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7046 - accuracy: 0.9631 - val_loss: 0.8059 - val_accuracy: 0.9241\n",
      "Epoch 106/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.7020 - accuracy: 0.9641 - val_loss: 0.8032 - val_accuracy: 0.9275\n",
      "Epoch 107/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6995 - accuracy: 0.9642 - val_loss: 0.8047 - val_accuracy: 0.9250\n",
      "Epoch 108/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.6968 - accuracy: 0.9642 - val_loss: 0.8016 - val_accuracy: 0.9250\n",
      "Epoch 109/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6941 - accuracy: 0.9653 - val_loss: 0.7992 - val_accuracy: 0.9249\n",
      "Epoch 110/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6914 - accuracy: 0.9658 - val_loss: 0.8000 - val_accuracy: 0.9253\n",
      "Epoch 111/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6899 - accuracy: 0.9649 - val_loss: 0.7999 - val_accuracy: 0.9233\n",
      "Epoch 112/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6883 - accuracy: 0.9647 - val_loss: 0.7933 - val_accuracy: 0.9260\n",
      "Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 4s 12ms/step - loss: 0.6853 - accuracy: 0.9656 - val_loss: 0.7962 - val_accuracy: 0.9260\n",
      "Epoch 114/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.6840 - accuracy: 0.9656 - val_loss: 0.7894 - val_accuracy: 0.9254\n",
      "Epoch 115/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.6810 - accuracy: 0.9665 - val_loss: 0.7880 - val_accuracy: 0.9270\n",
      "Epoch 116/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.6794 - accuracy: 0.9659 - val_loss: 0.7888 - val_accuracy: 0.9264\n",
      "Epoch 117/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.6776 - accuracy: 0.9664 - val_loss: 0.7851 - val_accuracy: 0.9257\n",
      "Epoch 118/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6753 - accuracy: 0.9669 - val_loss: 0.7906 - val_accuracy: 0.9233\n",
      "Epoch 119/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6736 - accuracy: 0.9666 - val_loss: 0.7832 - val_accuracy: 0.9252\n",
      "Epoch 120/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6715 - accuracy: 0.9666 - val_loss: 0.7788 - val_accuracy: 0.9281\n",
      "Epoch 121/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6681 - accuracy: 0.9677 - val_loss: 0.7853 - val_accuracy: 0.9228\n",
      "Epoch 122/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6675 - accuracy: 0.9669 - val_loss: 0.7788 - val_accuracy: 0.9242\n",
      "Epoch 123/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6654 - accuracy: 0.9682 - val_loss: 0.7732 - val_accuracy: 0.9262\n",
      "Epoch 124/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.6640 - accuracy: 0.9679 - val_loss: 0.7736 - val_accuracy: 0.9266\n",
      "Epoch 125/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6618 - accuracy: 0.9681 - val_loss: 0.7712 - val_accuracy: 0.9277\n",
      "Epoch 126/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6608 - accuracy: 0.9680 - val_loss: 0.7767 - val_accuracy: 0.9257\n",
      "Epoch 127/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.6586 - accuracy: 0.9676 - val_loss: 0.7699 - val_accuracy: 0.9272\n",
      "Epoch 128/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.6565 - accuracy: 0.9681 - val_loss: 0.7651 - val_accuracy: 0.9299\n",
      "Epoch 129/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6536 - accuracy: 0.9697 - val_loss: 0.7660 - val_accuracy: 0.9265\n",
      "Epoch 130/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6536 - accuracy: 0.9683 - val_loss: 0.7684 - val_accuracy: 0.9286\n",
      "Epoch 131/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6517 - accuracy: 0.9686 - val_loss: 0.7681 - val_accuracy: 0.9262\n",
      "Epoch 132/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6486 - accuracy: 0.9693 - val_loss: 0.7610 - val_accuracy: 0.9256\n",
      "Epoch 133/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6473 - accuracy: 0.9693 - val_loss: 0.7602 - val_accuracy: 0.9267\n",
      "Epoch 134/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6460 - accuracy: 0.9693 - val_loss: 0.7609 - val_accuracy: 0.9272\n",
      "Epoch 135/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6431 - accuracy: 0.9699 - val_loss: 0.7575 - val_accuracy: 0.9275\n",
      "Epoch 136/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6427 - accuracy: 0.9698 - val_loss: 0.7552 - val_accuracy: 0.9273\n",
      "Epoch 137/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6399 - accuracy: 0.9709 - val_loss: 0.7535 - val_accuracy: 0.9286\n",
      "Epoch 138/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6380 - accuracy: 0.9703 - val_loss: 0.7522 - val_accuracy: 0.9281\n",
      "Epoch 139/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6366 - accuracy: 0.9709 - val_loss: 0.7571 - val_accuracy: 0.9259\n",
      "Epoch 140/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6350 - accuracy: 0.9705 - val_loss: 0.7570 - val_accuracy: 0.9282\n",
      "Epoch 141/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6329 - accuracy: 0.9714 - val_loss: 0.7497 - val_accuracy: 0.9257\n",
      "Epoch 142/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6327 - accuracy: 0.9709 - val_loss: 0.7467 - val_accuracy: 0.9272\n",
      "Epoch 143/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6304 - accuracy: 0.9704 - val_loss: 0.7556 - val_accuracy: 0.9251\n",
      "Epoch 144/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6284 - accuracy: 0.9712 - val_loss: 0.7513 - val_accuracy: 0.9253\n",
      "Epoch 145/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6264 - accuracy: 0.9715 - val_loss: 0.7427 - val_accuracy: 0.9271\n",
      "Epoch 146/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6246 - accuracy: 0.9719 - val_loss: 0.7436 - val_accuracy: 0.9256\n",
      "Epoch 147/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6237 - accuracy: 0.9714 - val_loss: 0.7404 - val_accuracy: 0.9282\n",
      "Epoch 148/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6217 - accuracy: 0.9717 - val_loss: 0.7384 - val_accuracy: 0.9278\n",
      "Epoch 149/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6203 - accuracy: 0.9719 - val_loss: 0.7372 - val_accuracy: 0.9279\n",
      "Epoch 150/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6179 - accuracy: 0.9725 - val_loss: 0.7411 - val_accuracy: 0.9259\n",
      "Epoch 151/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6167 - accuracy: 0.9725 - val_loss: 0.7460 - val_accuracy: 0.9258\n",
      "Epoch 152/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6164 - accuracy: 0.9723 - val_loss: 0.7338 - val_accuracy: 0.9294\n",
      "Epoch 153/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6144 - accuracy: 0.9726 - val_loss: 0.7474 - val_accuracy: 0.9239\n",
      "Epoch 154/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6118 - accuracy: 0.9726 - val_loss: 0.7361 - val_accuracy: 0.9259\n",
      "Epoch 155/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6106 - accuracy: 0.9732 - val_loss: 0.7344 - val_accuracy: 0.9264\n",
      "Epoch 156/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6098 - accuracy: 0.9731 - val_loss: 0.7450 - val_accuracy: 0.9241\n",
      "Epoch 157/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6075 - accuracy: 0.9730 - val_loss: 0.7279 - val_accuracy: 0.9284\n",
      "Epoch 158/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6063 - accuracy: 0.9730 - val_loss: 0.7282 - val_accuracy: 0.9291\n",
      "Epoch 159/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6053 - accuracy: 0.9732 - val_loss: 0.7320 - val_accuracy: 0.9255\n",
      "Epoch 160/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6034 - accuracy: 0.9737 - val_loss: 0.7298 - val_accuracy: 0.9267\n",
      "Epoch 161/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6021 - accuracy: 0.9736 - val_loss: 0.7285 - val_accuracy: 0.9278\n",
      "Epoch 162/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6011 - accuracy: 0.9738 - val_loss: 0.7213 - val_accuracy: 0.9286\n",
      "Epoch 163/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.6008 - accuracy: 0.9735 - val_loss: 0.7270 - val_accuracy: 0.9267\n",
      "Epoch 164/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5964 - accuracy: 0.9754 - val_loss: 0.7300 - val_accuracy: 0.9249\n",
      "Epoch 165/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5961 - accuracy: 0.9748 - val_loss: 0.7224 - val_accuracy: 0.9291\n",
      "Epoch 166/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5954 - accuracy: 0.9749 - val_loss: 0.7315 - val_accuracy: 0.9253\n",
      "Epoch 167/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5940 - accuracy: 0.9745 - val_loss: 0.7361 - val_accuracy: 0.9245\n",
      "Epoch 168/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5933 - accuracy: 0.9746 - val_loss: 0.7194 - val_accuracy: 0.9300\n",
      "Epoch 169/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5911 - accuracy: 0.9749 - val_loss: 0.7160 - val_accuracy: 0.9280\n",
      "Epoch 170/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5898 - accuracy: 0.9752 - val_loss: 0.7210 - val_accuracy: 0.9283\n",
      "Epoch 171/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5893 - accuracy: 0.9751 - val_loss: 0.7129 - val_accuracy: 0.9285\n",
      "Epoch 172/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5871 - accuracy: 0.9752 - val_loss: 0.7194 - val_accuracy: 0.9273\n",
      "Epoch 173/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5864 - accuracy: 0.9750 - val_loss: 0.7149 - val_accuracy: 0.9267\n",
      "Epoch 174/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5849 - accuracy: 0.9755 - val_loss: 0.7161 - val_accuracy: 0.9278\n",
      "Epoch 175/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5845 - accuracy: 0.9749 - val_loss: 0.7133 - val_accuracy: 0.9282\n",
      "Epoch 176/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5818 - accuracy: 0.9755 - val_loss: 0.7103 - val_accuracy: 0.9296\n",
      "Epoch 177/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5803 - accuracy: 0.9762 - val_loss: 0.7130 - val_accuracy: 0.9275\n",
      "Epoch 178/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5791 - accuracy: 0.9768 - val_loss: 0.7072 - val_accuracy: 0.9293\n",
      "Epoch 179/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5780 - accuracy: 0.9767 - val_loss: 0.7064 - val_accuracy: 0.9301\n",
      "Epoch 180/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5773 - accuracy: 0.9759 - val_loss: 0.7068 - val_accuracy: 0.9298\n",
      "Epoch 181/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5754 - accuracy: 0.9771 - val_loss: 0.7125 - val_accuracy: 0.9269\n",
      "Epoch 182/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5750 - accuracy: 0.9762 - val_loss: 0.7079 - val_accuracy: 0.9276\n",
      "Epoch 183/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5740 - accuracy: 0.9766 - val_loss: 0.7061 - val_accuracy: 0.9276\n",
      "Epoch 184/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5722 - accuracy: 0.9768 - val_loss: 0.7026 - val_accuracy: 0.9301\n",
      "Epoch 185/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5713 - accuracy: 0.9766 - val_loss: 0.7030 - val_accuracy: 0.9288\n",
      "Epoch 186/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5708 - accuracy: 0.9765 - val_loss: 0.7062 - val_accuracy: 0.9283\n",
      "Epoch 187/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5688 - accuracy: 0.9773 - val_loss: 0.7046 - val_accuracy: 0.9273\n",
      "Epoch 188/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5684 - accuracy: 0.9769 - val_loss: 0.7069 - val_accuracy: 0.9280\n",
      "Epoch 189/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5667 - accuracy: 0.9768 - val_loss: 0.7031 - val_accuracy: 0.9272\n",
      "Epoch 190/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5662 - accuracy: 0.9772 - val_loss: 0.6987 - val_accuracy: 0.9283\n",
      "Epoch 191/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5644 - accuracy: 0.9779 - val_loss: 0.6999 - val_accuracy: 0.9288\n",
      "Epoch 192/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5640 - accuracy: 0.9776 - val_loss: 0.7013 - val_accuracy: 0.9257\n",
      "Epoch 193/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5621 - accuracy: 0.9773 - val_loss: 0.6948 - val_accuracy: 0.9288\n",
      "Epoch 194/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5608 - accuracy: 0.9779 - val_loss: 0.7094 - val_accuracy: 0.9242\n",
      "Epoch 195/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5598 - accuracy: 0.9781 - val_loss: 0.6960 - val_accuracy: 0.9293\n",
      "Epoch 196/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5591 - accuracy: 0.9782 - val_loss: 0.6945 - val_accuracy: 0.9301\n",
      "Epoch 197/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5583 - accuracy: 0.9781 - val_loss: 0.6973 - val_accuracy: 0.9270\n",
      "Epoch 198/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5571 - accuracy: 0.9782 - val_loss: 0.6871 - val_accuracy: 0.9312\n",
      "Epoch 199/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5568 - accuracy: 0.9780 - val_loss: 0.6944 - val_accuracy: 0.9308\n",
      "Epoch 200/300\n",
      "306/306 [==============================] - 4s 15ms/step - loss: 0.5551 - accuracy: 0.9783 - val_loss: 0.6926 - val_accuracy: 0.9283\n",
      "Epoch 201/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.5550 - accuracy: 0.9785 - val_loss: 0.6977 - val_accuracy: 0.9274\n",
      "Epoch 202/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5526 - accuracy: 0.9788 - val_loss: 0.6858 - val_accuracy: 0.9302\n",
      "Epoch 203/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5523 - accuracy: 0.9788 - val_loss: 0.6887 - val_accuracy: 0.9280\n",
      "Epoch 204/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5517 - accuracy: 0.9784 - val_loss: 0.6845 - val_accuracy: 0.9307\n",
      "Epoch 205/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5499 - accuracy: 0.9792 - val_loss: 0.6843 - val_accuracy: 0.9283\n",
      "Epoch 206/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5486 - accuracy: 0.9792 - val_loss: 0.6874 - val_accuracy: 0.9291\n",
      "Epoch 207/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5478 - accuracy: 0.9785 - val_loss: 0.6826 - val_accuracy: 0.9302\n",
      "Epoch 208/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5463 - accuracy: 0.9796 - val_loss: 0.6818 - val_accuracy: 0.9316\n",
      "Epoch 209/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5466 - accuracy: 0.9791 - val_loss: 0.6876 - val_accuracy: 0.9283\n",
      "Epoch 210/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5454 - accuracy: 0.9794 - val_loss: 0.6815 - val_accuracy: 0.9283\n",
      "Epoch 211/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5456 - accuracy: 0.9790 - val_loss: 0.6863 - val_accuracy: 0.9298\n",
      "Epoch 212/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5434 - accuracy: 0.9793 - val_loss: 0.6820 - val_accuracy: 0.9301\n",
      "Epoch 213/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5420 - accuracy: 0.9793 - val_loss: 0.6799 - val_accuracy: 0.9297\n",
      "Epoch 214/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5414 - accuracy: 0.9796 - val_loss: 0.6881 - val_accuracy: 0.9267\n",
      "Epoch 215/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5416 - accuracy: 0.9796 - val_loss: 0.6827 - val_accuracy: 0.9289\n",
      "Epoch 216/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5386 - accuracy: 0.9802 - val_loss: 0.6844 - val_accuracy: 0.9272\n",
      "Epoch 217/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5392 - accuracy: 0.9793 - val_loss: 0.6783 - val_accuracy: 0.9310\n",
      "Epoch 218/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5373 - accuracy: 0.9806 - val_loss: 0.6750 - val_accuracy: 0.9293\n",
      "Epoch 219/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5360 - accuracy: 0.9806 - val_loss: 0.6716 - val_accuracy: 0.9308\n",
      "Epoch 220/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5361 - accuracy: 0.9802 - val_loss: 0.6933 - val_accuracy: 0.9217\n",
      "Epoch 221/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5347 - accuracy: 0.9804 - val_loss: 0.6739 - val_accuracy: 0.9288\n",
      "Epoch 222/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5341 - accuracy: 0.9803 - val_loss: 0.6741 - val_accuracy: 0.9300\n",
      "Epoch 223/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5332 - accuracy: 0.9798 - val_loss: 0.6724 - val_accuracy: 0.9298\n",
      "Epoch 224/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5313 - accuracy: 0.9811 - val_loss: 0.6736 - val_accuracy: 0.9283\n",
      "Epoch 225/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 4s 12ms/step - loss: 0.5305 - accuracy: 0.9812 - val_loss: 0.6771 - val_accuracy: 0.9301\n",
      "Epoch 226/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5317 - accuracy: 0.9800 - val_loss: 0.6764 - val_accuracy: 0.9276\n",
      "Epoch 227/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.5292 - accuracy: 0.9811 - val_loss: 0.6673 - val_accuracy: 0.9305\n",
      "Epoch 228/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5288 - accuracy: 0.9809 - val_loss: 0.6727 - val_accuracy: 0.9296\n",
      "Epoch 229/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.5276 - accuracy: 0.9814 - val_loss: 0.6759 - val_accuracy: 0.9302\n",
      "Epoch 230/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.5261 - accuracy: 0.9812 - val_loss: 0.6700 - val_accuracy: 0.9283\n",
      "Epoch 231/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.5257 - accuracy: 0.9817 - val_loss: 0.6662 - val_accuracy: 0.9324\n",
      "Epoch 232/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5259 - accuracy: 0.9810 - val_loss: 0.6660 - val_accuracy: 0.9304\n",
      "Epoch 233/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5235 - accuracy: 0.9817 - val_loss: 0.6669 - val_accuracy: 0.9283\n",
      "Epoch 234/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.5243 - accuracy: 0.9813 - val_loss: 0.6632 - val_accuracy: 0.9299\n",
      "Epoch 235/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5221 - accuracy: 0.9820 - val_loss: 0.6684 - val_accuracy: 0.9290\n",
      "Epoch 236/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.5217 - accuracy: 0.9819 - val_loss: 0.6665 - val_accuracy: 0.9293\n",
      "Epoch 237/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5205 - accuracy: 0.9820 - val_loss: 0.6681 - val_accuracy: 0.9263\n",
      "Epoch 238/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5210 - accuracy: 0.9813 - val_loss: 0.6617 - val_accuracy: 0.9294\n",
      "Epoch 239/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5187 - accuracy: 0.9816 - val_loss: 0.6640 - val_accuracy: 0.9304\n",
      "Epoch 240/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5184 - accuracy: 0.9823 - val_loss: 0.6673 - val_accuracy: 0.9306\n",
      "Epoch 241/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5176 - accuracy: 0.9817 - val_loss: 0.6589 - val_accuracy: 0.9326\n",
      "Epoch 242/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5172 - accuracy: 0.9821 - val_loss: 0.6596 - val_accuracy: 0.9312\n",
      "Epoch 243/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5166 - accuracy: 0.9811 - val_loss: 0.6686 - val_accuracy: 0.9284\n",
      "Epoch 244/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5166 - accuracy: 0.9817 - val_loss: 0.6607 - val_accuracy: 0.9301\n",
      "Epoch 245/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5158 - accuracy: 0.9811 - val_loss: 0.6606 - val_accuracy: 0.9306\n",
      "Epoch 246/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5132 - accuracy: 0.9829 - val_loss: 0.6571 - val_accuracy: 0.9302\n",
      "Epoch 247/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5129 - accuracy: 0.9823 - val_loss: 0.6564 - val_accuracy: 0.9324\n",
      "Epoch 248/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5122 - accuracy: 0.9823 - val_loss: 0.6650 - val_accuracy: 0.9272\n",
      "Epoch 249/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5119 - accuracy: 0.9824 - val_loss: 0.6661 - val_accuracy: 0.9276\n",
      "Epoch 250/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5119 - accuracy: 0.9819 - val_loss: 0.6563 - val_accuracy: 0.9307\n",
      "Epoch 251/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5101 - accuracy: 0.9823 - val_loss: 0.6616 - val_accuracy: 0.9283\n",
      "Epoch 252/300\n",
      "306/306 [==============================] - 4s 15ms/step - loss: 0.5104 - accuracy: 0.9823 - val_loss: 0.6542 - val_accuracy: 0.9299\n",
      "Epoch 253/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5076 - accuracy: 0.9830 - val_loss: 0.6560 - val_accuracy: 0.9288\n",
      "Epoch 254/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5081 - accuracy: 0.9828 - val_loss: 0.6525 - val_accuracy: 0.9308\n",
      "Epoch 255/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5065 - accuracy: 0.9830 - val_loss: 0.6545 - val_accuracy: 0.9303\n",
      "Epoch 256/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5060 - accuracy: 0.9832 - val_loss: 0.6561 - val_accuracy: 0.9308\n",
      "Epoch 257/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5065 - accuracy: 0.9827 - val_loss: 0.6547 - val_accuracy: 0.9283\n",
      "Epoch 258/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5039 - accuracy: 0.9834 - val_loss: 0.6545 - val_accuracy: 0.9288\n",
      "Epoch 259/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5040 - accuracy: 0.9833 - val_loss: 0.6480 - val_accuracy: 0.9336\n",
      "Epoch 260/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5026 - accuracy: 0.9831 - val_loss: 0.6494 - val_accuracy: 0.9304\n",
      "Epoch 261/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.5021 - accuracy: 0.9835 - val_loss: 0.6519 - val_accuracy: 0.9309\n",
      "Epoch 262/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5023 - accuracy: 0.9830 - val_loss: 0.6557 - val_accuracy: 0.9298\n",
      "Epoch 263/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.5016 - accuracy: 0.9833 - val_loss: 0.6526 - val_accuracy: 0.9309\n",
      "Epoch 264/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.5005 - accuracy: 0.9835 - val_loss: 0.6523 - val_accuracy: 0.9280\n",
      "Epoch 265/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.5000 - accuracy: 0.9833 - val_loss: 0.6545 - val_accuracy: 0.9295\n",
      "Epoch 266/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4987 - accuracy: 0.9838 - val_loss: 0.6509 - val_accuracy: 0.9292\n",
      "Epoch 267/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.4981 - accuracy: 0.9835 - val_loss: 0.6440 - val_accuracy: 0.9324\n",
      "Epoch 268/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.4973 - accuracy: 0.9840 - val_loss: 0.6449 - val_accuracy: 0.9320\n",
      "Epoch 269/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4963 - accuracy: 0.9846 - val_loss: 0.6439 - val_accuracy: 0.9323\n",
      "Epoch 270/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4963 - accuracy: 0.9841 - val_loss: 0.6465 - val_accuracy: 0.9305\n",
      "Epoch 271/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4965 - accuracy: 0.9832 - val_loss: 0.6442 - val_accuracy: 0.9309\n",
      "Epoch 272/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4952 - accuracy: 0.9839 - val_loss: 0.6428 - val_accuracy: 0.9325\n",
      "Epoch 273/300\n",
      "306/306 [==============================] - 4s 12ms/step - loss: 0.4928 - accuracy: 0.9850 - val_loss: 0.6466 - val_accuracy: 0.9286\n",
      "Epoch 274/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4944 - accuracy: 0.9835 - val_loss: 0.6440 - val_accuracy: 0.9313\n",
      "Epoch 275/300\n",
      "306/306 [==============================] - 5s 16ms/step - loss: 0.4928 - accuracy: 0.9839 - val_loss: 0.6510 - val_accuracy: 0.9269\n",
      "Epoch 276/300\n",
      "306/306 [==============================] - 5s 15ms/step - loss: 0.4921 - accuracy: 0.9842 - val_loss: 0.6385 - val_accuracy: 0.9331\n",
      "Epoch 277/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4917 - accuracy: 0.9846 - val_loss: 0.6492 - val_accuracy: 0.9283\n",
      "Epoch 278/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4902 - accuracy: 0.9849 - val_loss: 0.6481 - val_accuracy: 0.9277\n",
      "Epoch 279/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4932 - accuracy: 0.9829 - val_loss: 0.6612 - val_accuracy: 0.9248\n",
      "Epoch 280/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4895 - accuracy: 0.9846 - val_loss: 0.6375 - val_accuracy: 0.9317\n",
      "Epoch 281/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4904 - accuracy: 0.9836 - val_loss: 0.6419 - val_accuracy: 0.9304\n",
      "Epoch 282/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4880 - accuracy: 0.9848 - val_loss: 0.6401 - val_accuracy: 0.9317\n",
      "Epoch 283/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4872 - accuracy: 0.9852 - val_loss: 0.6366 - val_accuracy: 0.9320\n",
      "Epoch 284/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4867 - accuracy: 0.9848 - val_loss: 0.6387 - val_accuracy: 0.9322\n",
      "Epoch 285/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4865 - accuracy: 0.9852 - val_loss: 0.6400 - val_accuracy: 0.9308\n",
      "Epoch 286/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4856 - accuracy: 0.9847 - val_loss: 0.6404 - val_accuracy: 0.9295\n",
      "Epoch 287/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4845 - accuracy: 0.9853 - val_loss: 0.6413 - val_accuracy: 0.9307\n",
      "Epoch 288/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4851 - accuracy: 0.9846 - val_loss: 0.6446 - val_accuracy: 0.9273\n",
      "Epoch 289/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4843 - accuracy: 0.9851 - val_loss: 0.6364 - val_accuracy: 0.9311\n",
      "Epoch 290/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4827 - accuracy: 0.9854 - val_loss: 0.6361 - val_accuracy: 0.9311\n",
      "Epoch 291/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4836 - accuracy: 0.9847 - val_loss: 0.6384 - val_accuracy: 0.9309\n",
      "Epoch 292/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4833 - accuracy: 0.9843 - val_loss: 0.6362 - val_accuracy: 0.9311\n",
      "Epoch 293/300\n",
      "306/306 [==============================] - 4s 15ms/step - loss: 0.4818 - accuracy: 0.9854 - val_loss: 0.6398 - val_accuracy: 0.9276\n",
      "Epoch 294/300\n",
      "306/306 [==============================] - 4s 14ms/step - loss: 0.4819 - accuracy: 0.9844 - val_loss: 0.6381 - val_accuracy: 0.9323\n",
      "Epoch 295/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4817 - accuracy: 0.9843 - val_loss: 0.6333 - val_accuracy: 0.9292\n",
      "Epoch 296/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4785 - accuracy: 0.9860 - val_loss: 0.6313 - val_accuracy: 0.9333\n",
      "Epoch 297/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4785 - accuracy: 0.9854 - val_loss: 0.6392 - val_accuracy: 0.9291\n",
      "Epoch 298/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4781 - accuracy: 0.9855 - val_loss: 0.6322 - val_accuracy: 0.9297\n",
      "Epoch 299/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4772 - accuracy: 0.9863 - val_loss: 0.6358 - val_accuracy: 0.9297\n",
      "Epoch 300/300\n",
      "306/306 [==============================] - 4s 13ms/step - loss: 0.4767 - accuracy: 0.9858 - val_loss: 0.6315 - val_accuracy: 0.9312\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.layers import Activation\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(400, input_dim=1024,  kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
    "                                      bias_regularizer=regularizers.l2(1e-3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dense(200,  kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
    "                                      bias_regularizer=regularizers.l2(1e-3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Dense(100,   kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
    "                                      bias_regularizer=regularizers.l2(1e-3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Dense(50,kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
    "                                      bias_regularizer=regularizers.l2(1e-3)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.00019)\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "History=model.fit(Train_X,Train_Y,validation_data=(Test_X, Test_Y), epochs=300, batch_size=256,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(History.history['accuracy'])\n",
    "plt.plot(History.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 3.9853 - accuracy: 0.4252 - val_loss: 2.8734 - val_accuracy: 0.6261\n",
      "Epoch 2/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 2.6315 - accuracy: 0.6676 - val_loss: 2.4125 - val_accuracy: 0.7049\n",
      "Epoch 3/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 2.2911 - accuracy: 0.7199 - val_loss: 2.1560 - val_accuracy: 0.7423\n",
      "Epoch 4/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 2.0654 - accuracy: 0.7540 - val_loss: 1.9694 - val_accuracy: 0.7628\n",
      "Epoch 5/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.8910 - accuracy: 0.7806 - val_loss: 1.8251 - val_accuracy: 0.7839\n",
      "Epoch 6/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 1.7534 - accuracy: 0.8012 - val_loss: 1.6959 - val_accuracy: 0.8068\n",
      "Epoch 7/300\n",
      "611/611 [==============================] - 6s 9ms/step - loss: 1.6397 - accuracy: 0.8175 - val_loss: 1.5997 - val_accuracy: 0.8181\n",
      "Epoch 8/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.5472 - accuracy: 0.8321 - val_loss: 1.5257 - val_accuracy: 0.8257\n",
      "Epoch 9/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.4714 - accuracy: 0.8433 - val_loss: 1.4506 - val_accuracy: 0.8404\n",
      "Epoch 10/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 1.4068 - accuracy: 0.8524 - val_loss: 1.4060 - val_accuracy: 0.8434\n",
      "Epoch 11/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 1.3529 - accuracy: 0.8599 - val_loss: 1.3441 - val_accuracy: 0.8560\n",
      "Epoch 12/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 1.3056 - accuracy: 0.8674 - val_loss: 1.3066 - val_accuracy: 0.8581\n",
      "Epoch 13/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 1.2640 - accuracy: 0.8735 - val_loss: 1.2717 - val_accuracy: 0.8652\n",
      "Epoch 14/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.2272 - accuracy: 0.8785 - val_loss: 1.2490 - val_accuracy: 0.8638\n",
      "Epoch 15/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.1945 - accuracy: 0.8840 - val_loss: 1.2039 - val_accuracy: 0.8740\n",
      "Epoch 16/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.1648 - accuracy: 0.8869 - val_loss: 1.1818 - val_accuracy: 0.8752\n",
      "Epoch 17/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.1378 - accuracy: 0.8901 - val_loss: 1.1570 - val_accuracy: 0.8775\n",
      "Epoch 18/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.1115 - accuracy: 0.8936 - val_loss: 1.1399 - val_accuracy: 0.8797\n",
      "Epoch 19/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.0902 - accuracy: 0.8976 - val_loss: 1.1096 - val_accuracy: 0.8886\n",
      "Epoch 20/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.0674 - accuracy: 0.9004 - val_loss: 1.0842 - val_accuracy: 0.8907\n",
      "Epoch 21/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.0465 - accuracy: 0.9027 - val_loss: 1.0786 - val_accuracy: 0.8888\n",
      "Epoch 22/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.0288 - accuracy: 0.9044 - val_loss: 1.0543 - val_accuracy: 0.8922\n",
      "Epoch 23/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 1.0112 - accuracy: 0.9068 - val_loss: 1.0367 - val_accuracy: 0.8957\n",
      "Epoch 24/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.9925 - accuracy: 0.9104 - val_loss: 1.0219 - val_accuracy: 0.8943\n",
      "Epoch 25/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.9778 - accuracy: 0.9120 - val_loss: 1.0118 - val_accuracy: 0.8954\n",
      "Epoch 26/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.9626 - accuracy: 0.9127 - val_loss: 0.9991 - val_accuracy: 0.8990\n",
      "Epoch 27/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.9484 - accuracy: 0.9152 - val_loss: 0.9813 - val_accuracy: 0.9001\n",
      "Epoch 28/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.9347 - accuracy: 0.9163 - val_loss: 0.9720 - val_accuracy: 0.9017\n",
      "Epoch 29/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.9235 - accuracy: 0.9180 - val_loss: 0.9622 - val_accuracy: 0.9008\n",
      "Epoch 30/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.9104 - accuracy: 0.9197 - val_loss: 0.9517 - val_accuracy: 0.9024\n",
      "Epoch 31/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8985 - accuracy: 0.9211 - val_loss: 0.9386 - val_accuracy: 0.9057\n",
      "Epoch 32/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.8881 - accuracy: 0.9223 - val_loss: 0.9277 - val_accuracy: 0.9067\n",
      "Epoch 33/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8776 - accuracy: 0.9237 - val_loss: 0.9203 - val_accuracy: 0.9060\n",
      "Epoch 34/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8678 - accuracy: 0.9251 - val_loss: 0.9155 - val_accuracy: 0.9055\n",
      "Epoch 35/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8594 - accuracy: 0.9261 - val_loss: 0.9069 - val_accuracy: 0.9070\n",
      "Epoch 36/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8498 - accuracy: 0.9269 - val_loss: 0.8979 - val_accuracy: 0.9083\n",
      "Epoch 37/300\n",
      "611/611 [==============================] - 6s 9ms/step - loss: 0.8414 - accuracy: 0.9287 - val_loss: 0.8967 - val_accuracy: 0.9077\n",
      "Epoch 38/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8343 - accuracy: 0.9289 - val_loss: 0.8831 - val_accuracy: 0.9091\n",
      "Epoch 39/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8268 - accuracy: 0.9299 - val_loss: 0.8760 - val_accuracy: 0.9111\n",
      "Epoch 40/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8205 - accuracy: 0.9305 - val_loss: 0.8787 - val_accuracy: 0.9071\n",
      "Epoch 41/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8135 - accuracy: 0.9325 - val_loss: 0.8633 - val_accuracy: 0.9117\n",
      "Epoch 42/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8067 - accuracy: 0.9326 - val_loss: 0.8591 - val_accuracy: 0.9097\n",
      "Epoch 43/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.8010 - accuracy: 0.9339 - val_loss: 0.8568 - val_accuracy: 0.9121\n",
      "Epoch 44/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7958 - accuracy: 0.9338 - val_loss: 0.8530 - val_accuracy: 0.9103\n",
      "Epoch 45/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7903 - accuracy: 0.9347 - val_loss: 0.8482 - val_accuracy: 0.9117\n",
      "Epoch 46/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7849 - accuracy: 0.9365 - val_loss: 0.8398 - val_accuracy: 0.9131\n",
      "Epoch 47/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7798 - accuracy: 0.9368 - val_loss: 0.8358 - val_accuracy: 0.9129\n",
      "Epoch 48/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.7743 - accuracy: 0.9376 - val_loss: 0.8321 - val_accuracy: 0.9130\n",
      "Epoch 49/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7702 - accuracy: 0.9377 - val_loss: 0.8266 - val_accuracy: 0.9146\n",
      "Epoch 50/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7659 - accuracy: 0.9385 - val_loss: 0.8246 - val_accuracy: 0.9143\n",
      "Epoch 51/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7619 - accuracy: 0.9387 - val_loss: 0.8264 - val_accuracy: 0.9132\n",
      "Epoch 52/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7566 - accuracy: 0.9395 - val_loss: 0.8178 - val_accuracy: 0.9151\n",
      "Epoch 53/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7524 - accuracy: 0.9404 - val_loss: 0.8114 - val_accuracy: 0.9170\n",
      "Epoch 54/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7485 - accuracy: 0.9410 - val_loss: 0.8158 - val_accuracy: 0.9130\n",
      "Epoch 55/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7454 - accuracy: 0.9411 - val_loss: 0.8058 - val_accuracy: 0.9172\n",
      "Epoch 56/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7414 - accuracy: 0.9416 - val_loss: 0.8045 - val_accuracy: 0.9162\n",
      "Epoch 57/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7385 - accuracy: 0.9417 - val_loss: 0.8053 - val_accuracy: 0.9153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7342 - accuracy: 0.9427 - val_loss: 0.8007 - val_accuracy: 0.9183\n",
      "Epoch 59/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7311 - accuracy: 0.9429 - val_loss: 0.7975 - val_accuracy: 0.9167\n",
      "Epoch 60/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7280 - accuracy: 0.9440 - val_loss: 0.7974 - val_accuracy: 0.9167\n",
      "Epoch 61/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7246 - accuracy: 0.9443 - val_loss: 0.7943 - val_accuracy: 0.9174\n",
      "Epoch 62/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7226 - accuracy: 0.9434 - val_loss: 0.7886 - val_accuracy: 0.9180\n",
      "Epoch 63/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7181 - accuracy: 0.9448 - val_loss: 0.7894 - val_accuracy: 0.9172\n",
      "Epoch 64/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7159 - accuracy: 0.9459 - val_loss: 0.7802 - val_accuracy: 0.9181\n",
      "Epoch 65/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7139 - accuracy: 0.9452 - val_loss: 0.7992 - val_accuracy: 0.9138\n",
      "Epoch 66/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.7104 - accuracy: 0.9457 - val_loss: 0.7773 - val_accuracy: 0.9191\n",
      "Epoch 67/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.7065 - accuracy: 0.9473 - val_loss: 0.7763 - val_accuracy: 0.9180\n",
      "Epoch 68/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7047 - accuracy: 0.9464 - val_loss: 0.7804 - val_accuracy: 0.9177\n",
      "Epoch 69/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.7026 - accuracy: 0.9476 - val_loss: 0.7736 - val_accuracy: 0.9190\n",
      "Epoch 70/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6989 - accuracy: 0.9475 - val_loss: 0.7716 - val_accuracy: 0.9187\n",
      "Epoch 71/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6979 - accuracy: 0.9479 - val_loss: 0.7745 - val_accuracy: 0.9168\n",
      "Epoch 72/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6945 - accuracy: 0.9486 - val_loss: 0.7708 - val_accuracy: 0.9199\n",
      "Epoch 73/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.6923 - accuracy: 0.9484 - val_loss: 0.7685 - val_accuracy: 0.9194\n",
      "Epoch 74/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6900 - accuracy: 0.9492 - val_loss: 0.7701 - val_accuracy: 0.9179\n",
      "Epoch 75/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6869 - accuracy: 0.9496 - val_loss: 0.7676 - val_accuracy: 0.9178\n",
      "Epoch 76/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6855 - accuracy: 0.9495 - val_loss: 0.7609 - val_accuracy: 0.9195\n",
      "Epoch 77/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.6834 - accuracy: 0.9493 - val_loss: 0.7624 - val_accuracy: 0.9180\n",
      "Epoch 78/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6814 - accuracy: 0.9500 - val_loss: 0.7638 - val_accuracy: 0.9199\n",
      "Epoch 79/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6794 - accuracy: 0.9507 - val_loss: 0.7535 - val_accuracy: 0.9199\n",
      "Epoch 80/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6765 - accuracy: 0.9506 - val_loss: 0.7507 - val_accuracy: 0.9206\n",
      "Epoch 81/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6755 - accuracy: 0.9511 - val_loss: 0.7551 - val_accuracy: 0.9194\n",
      "Epoch 82/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6741 - accuracy: 0.9512 - val_loss: 0.7519 - val_accuracy: 0.9204\n",
      "Epoch 83/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6713 - accuracy: 0.9510 - val_loss: 0.7466 - val_accuracy: 0.9219\n",
      "Epoch 84/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6698 - accuracy: 0.9515 - val_loss: 0.7527 - val_accuracy: 0.9188\n",
      "Epoch 85/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6684 - accuracy: 0.9515 - val_loss: 0.7407 - val_accuracy: 0.9242\n",
      "Epoch 86/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6666 - accuracy: 0.9518 - val_loss: 0.7443 - val_accuracy: 0.9210\n",
      "Epoch 87/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6642 - accuracy: 0.9523 - val_loss: 0.7417 - val_accuracy: 0.9236\n",
      "Epoch 88/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6629 - accuracy: 0.9527 - val_loss: 0.7398 - val_accuracy: 0.9213\n",
      "Epoch 89/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6606 - accuracy: 0.9524 - val_loss: 0.7396 - val_accuracy: 0.9228\n",
      "Epoch 90/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6597 - accuracy: 0.9526 - val_loss: 0.7345 - val_accuracy: 0.9233\n",
      "Epoch 91/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6586 - accuracy: 0.9531 - val_loss: 0.7359 - val_accuracy: 0.9219\n",
      "Epoch 92/300\n",
      "611/611 [==============================] - 6s 10ms/step - loss: 0.6563 - accuracy: 0.9534 - val_loss: 0.7454 - val_accuracy: 0.9199\n",
      "Epoch 93/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6544 - accuracy: 0.9542 - val_loss: 0.7337 - val_accuracy: 0.9249\n",
      "Epoch 94/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6519 - accuracy: 0.9540 - val_loss: 0.7322 - val_accuracy: 0.9210\n",
      "Epoch 95/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.6513 - accuracy: 0.9542 - val_loss: 0.7302 - val_accuracy: 0.9226\n",
      "Epoch 96/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6498 - accuracy: 0.9546 - val_loss: 0.7310 - val_accuracy: 0.9224\n",
      "Epoch 97/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6480 - accuracy: 0.9548 - val_loss: 0.7289 - val_accuracy: 0.9226\n",
      "Epoch 98/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6471 - accuracy: 0.9540 - val_loss: 0.7329 - val_accuracy: 0.9211\n",
      "Epoch 99/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6459 - accuracy: 0.9547 - val_loss: 0.7291 - val_accuracy: 0.9226\n",
      "Epoch 100/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6442 - accuracy: 0.9553 - val_loss: 0.7235 - val_accuracy: 0.9241\n",
      "Epoch 101/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6423 - accuracy: 0.9553 - val_loss: 0.7225 - val_accuracy: 0.9254\n",
      "Epoch 102/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6419 - accuracy: 0.9557 - val_loss: 0.7215 - val_accuracy: 0.9246\n",
      "Epoch 103/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6402 - accuracy: 0.9549 - val_loss: 0.7221 - val_accuracy: 0.9246\n",
      "Epoch 104/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6387 - accuracy: 0.9564 - val_loss: 0.7275 - val_accuracy: 0.9236\n",
      "Epoch 105/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6377 - accuracy: 0.9563 - val_loss: 0.7169 - val_accuracy: 0.9275\n",
      "Epoch 106/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.6358 - accuracy: 0.9565 - val_loss: 0.7149 - val_accuracy: 0.9259\n",
      "Epoch 107/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6349 - accuracy: 0.9564 - val_loss: 0.7163 - val_accuracy: 0.9245\n",
      "Epoch 108/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6335 - accuracy: 0.9570 - val_loss: 0.7176 - val_accuracy: 0.9241\n",
      "Epoch 109/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.6328 - accuracy: 0.9566 - val_loss: 0.7177 - val_accuracy: 0.9240\n",
      "Epoch 110/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6315 - accuracy: 0.9569 - val_loss: 0.7104 - val_accuracy: 0.9267\n",
      "Epoch 111/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6306 - accuracy: 0.9569 - val_loss: 0.7113 - val_accuracy: 0.9257\n",
      "Epoch 112/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6298 - accuracy: 0.9573 - val_loss: 0.7134 - val_accuracy: 0.9267\n",
      "Epoch 113/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6282 - accuracy: 0.9567 - val_loss: 0.7145 - val_accuracy: 0.9255\n",
      "Epoch 114/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6276 - accuracy: 0.9572 - val_loss: 0.7135 - val_accuracy: 0.9251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6251 - accuracy: 0.9579 - val_loss: 0.7096 - val_accuracy: 0.9254\n",
      "Epoch 116/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6253 - accuracy: 0.9582 - val_loss: 0.7088 - val_accuracy: 0.9274\n",
      "Epoch 117/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6237 - accuracy: 0.9581 - val_loss: 0.7151 - val_accuracy: 0.9227\n",
      "Epoch 118/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6233 - accuracy: 0.9579 - val_loss: 0.7053 - val_accuracy: 0.9280\n",
      "Epoch 119/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6219 - accuracy: 0.9582 - val_loss: 0.7111 - val_accuracy: 0.9260\n",
      "Epoch 120/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6206 - accuracy: 0.9589 - val_loss: 0.7089 - val_accuracy: 0.9260\n",
      "Epoch 121/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6189 - accuracy: 0.9582 - val_loss: 0.7053 - val_accuracy: 0.9281\n",
      "Epoch 122/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6192 - accuracy: 0.9589 - val_loss: 0.7078 - val_accuracy: 0.9264\n",
      "Epoch 123/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6183 - accuracy: 0.9586 - val_loss: 0.7001 - val_accuracy: 0.9270\n",
      "Epoch 124/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6163 - accuracy: 0.9598 - val_loss: 0.7070 - val_accuracy: 0.9242\n",
      "Epoch 125/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6169 - accuracy: 0.9588 - val_loss: 0.7031 - val_accuracy: 0.9242\n",
      "Epoch 126/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6156 - accuracy: 0.9592 - val_loss: 0.7039 - val_accuracy: 0.9258\n",
      "Epoch 127/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6142 - accuracy: 0.9598 - val_loss: 0.7012 - val_accuracy: 0.9272\n",
      "Epoch 128/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6135 - accuracy: 0.9597 - val_loss: 0.7074 - val_accuracy: 0.9265\n",
      "Epoch 129/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6121 - accuracy: 0.9601 - val_loss: 0.6971 - val_accuracy: 0.9275\n",
      "Epoch 130/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6116 - accuracy: 0.9597 - val_loss: 0.7045 - val_accuracy: 0.9258\n",
      "Epoch 131/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6105 - accuracy: 0.9603 - val_loss: 0.6989 - val_accuracy: 0.9268\n",
      "Epoch 132/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6097 - accuracy: 0.9607 - val_loss: 0.6983 - val_accuracy: 0.9279\n",
      "Epoch 133/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6101 - accuracy: 0.9597 - val_loss: 0.6946 - val_accuracy: 0.9270\n",
      "Epoch 134/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6082 - accuracy: 0.9613 - val_loss: 0.6934 - val_accuracy: 0.9275\n",
      "Epoch 135/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.6074 - accuracy: 0.9608 - val_loss: 0.6998 - val_accuracy: 0.9259\n",
      "Epoch 136/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6067 - accuracy: 0.9612 - val_loss: 0.6985 - val_accuracy: 0.9260\n",
      "Epoch 137/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6058 - accuracy: 0.9605 - val_loss: 0.6957 - val_accuracy: 0.9264\n",
      "Epoch 138/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6053 - accuracy: 0.9604 - val_loss: 0.6960 - val_accuracy: 0.9264\n",
      "Epoch 139/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6043 - accuracy: 0.9611 - val_loss: 0.6955 - val_accuracy: 0.9261\n",
      "Epoch 140/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6031 - accuracy: 0.9612 - val_loss: 0.6922 - val_accuracy: 0.9285\n",
      "Epoch 141/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6021 - accuracy: 0.9612 - val_loss: 0.6940 - val_accuracy: 0.9272\n",
      "Epoch 142/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6015 - accuracy: 0.9614 - val_loss: 0.6942 - val_accuracy: 0.9247\n",
      "Epoch 143/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6005 - accuracy: 0.9620 - val_loss: 0.6922 - val_accuracy: 0.9268\n",
      "Epoch 144/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.6003 - accuracy: 0.9617 - val_loss: 0.6905 - val_accuracy: 0.9291\n",
      "Epoch 145/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5996 - accuracy: 0.9628 - val_loss: 0.6864 - val_accuracy: 0.9285\n",
      "Epoch 146/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5986 - accuracy: 0.9621 - val_loss: 0.6858 - val_accuracy: 0.9282\n",
      "Epoch 147/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5980 - accuracy: 0.9623 - val_loss: 0.6938 - val_accuracy: 0.9274\n",
      "Epoch 148/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5970 - accuracy: 0.9624 - val_loss: 0.6844 - val_accuracy: 0.9300\n",
      "Epoch 149/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5967 - accuracy: 0.9624 - val_loss: 0.6885 - val_accuracy: 0.9278\n",
      "Epoch 150/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5952 - accuracy: 0.9627 - val_loss: 0.7029 - val_accuracy: 0.9228\n",
      "Epoch 151/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5960 - accuracy: 0.9622 - val_loss: 0.6872 - val_accuracy: 0.9272\n",
      "Epoch 152/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5941 - accuracy: 0.9631 - val_loss: 0.6863 - val_accuracy: 0.9270\n",
      "Epoch 153/300\n",
      "611/611 [==============================] - 6s 10ms/step - loss: 0.5943 - accuracy: 0.9623 - val_loss: 0.6905 - val_accuracy: 0.9262\n",
      "Epoch 154/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5930 - accuracy: 0.9630 - val_loss: 0.6916 - val_accuracy: 0.9254\n",
      "Epoch 155/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5924 - accuracy: 0.9632 - val_loss: 0.6966 - val_accuracy: 0.9257\n",
      "Epoch 156/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5922 - accuracy: 0.9630 - val_loss: 0.6807 - val_accuracy: 0.9281\n",
      "Epoch 157/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5914 - accuracy: 0.9634 - val_loss: 0.6892 - val_accuracy: 0.9267\n",
      "Epoch 158/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5907 - accuracy: 0.9632 - val_loss: 0.6841 - val_accuracy: 0.9297\n",
      "Epoch 159/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5896 - accuracy: 0.9634 - val_loss: 0.6833 - val_accuracy: 0.9282\n",
      "Epoch 160/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5885 - accuracy: 0.9641 - val_loss: 0.6919 - val_accuracy: 0.9257\n",
      "Epoch 161/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5881 - accuracy: 0.9636 - val_loss: 0.6820 - val_accuracy: 0.9291\n",
      "Epoch 162/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5883 - accuracy: 0.9638 - val_loss: 0.6851 - val_accuracy: 0.9272\n",
      "Epoch 163/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5866 - accuracy: 0.9646 - val_loss: 0.6789 - val_accuracy: 0.9289\n",
      "Epoch 164/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5868 - accuracy: 0.9641 - val_loss: 0.6876 - val_accuracy: 0.9259\n",
      "Epoch 165/300\n",
      "611/611 [==============================] - 7s 11ms/step - loss: 0.5855 - accuracy: 0.9638 - val_loss: 0.6790 - val_accuracy: 0.9290\n",
      "Epoch 166/300\n",
      "611/611 [==============================] - 7s 12ms/step - loss: 0.5848 - accuracy: 0.9642 - val_loss: 0.6804 - val_accuracy: 0.9284\n",
      "Epoch 167/300\n",
      "611/611 [==============================] - 6s 10ms/step - loss: 0.5839 - accuracy: 0.9649 - val_loss: 0.6786 - val_accuracy: 0.9280\n",
      "Epoch 168/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5841 - accuracy: 0.9646 - val_loss: 0.6818 - val_accuracy: 0.9278\n",
      "Epoch 169/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5831 - accuracy: 0.9645 - val_loss: 0.6741 - val_accuracy: 0.9307\n",
      "Epoch 170/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5820 - accuracy: 0.9649 - val_loss: 0.6781 - val_accuracy: 0.9295\n",
      "Epoch 171/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5826 - accuracy: 0.9650 - val_loss: 0.6753 - val_accuracy: 0.9306\n",
      "Epoch 172/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5815 - accuracy: 0.9649 - val_loss: 0.6778 - val_accuracy: 0.9295\n",
      "Epoch 173/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5809 - accuracy: 0.9649 - val_loss: 0.6764 - val_accuracy: 0.9289\n",
      "Epoch 174/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5801 - accuracy: 0.9651 - val_loss: 0.6736 - val_accuracy: 0.9304\n",
      "Epoch 175/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5796 - accuracy: 0.9649 - val_loss: 0.6927 - val_accuracy: 0.9234\n",
      "Epoch 176/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5795 - accuracy: 0.9647 - val_loss: 0.6736 - val_accuracy: 0.9302\n",
      "Epoch 177/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5789 - accuracy: 0.9655 - val_loss: 0.6761 - val_accuracy: 0.9283\n",
      "Epoch 178/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5786 - accuracy: 0.9648 - val_loss: 0.6783 - val_accuracy: 0.9269\n",
      "Epoch 179/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5782 - accuracy: 0.9652 - val_loss: 0.6746 - val_accuracy: 0.9305\n",
      "Epoch 180/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5781 - accuracy: 0.9657 - val_loss: 0.6727 - val_accuracy: 0.9304\n",
      "Epoch 181/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5768 - accuracy: 0.9657 - val_loss: 0.6727 - val_accuracy: 0.9296\n",
      "Epoch 182/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5763 - accuracy: 0.9658 - val_loss: 0.6797 - val_accuracy: 0.9283\n",
      "Epoch 183/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5749 - accuracy: 0.9665 - val_loss: 0.6787 - val_accuracy: 0.9307\n",
      "Epoch 184/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5750 - accuracy: 0.9662 - val_loss: 0.6784 - val_accuracy: 0.9269\n",
      "Epoch 185/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5741 - accuracy: 0.9662 - val_loss: 0.6733 - val_accuracy: 0.9301\n",
      "Epoch 186/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5740 - accuracy: 0.9664 - val_loss: 0.6690 - val_accuracy: 0.9293\n",
      "Epoch 187/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5737 - accuracy: 0.9663 - val_loss: 0.6819 - val_accuracy: 0.9267\n",
      "Epoch 188/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5736 - accuracy: 0.9663 - val_loss: 0.6703 - val_accuracy: 0.9299\n",
      "Epoch 189/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5723 - accuracy: 0.9668 - val_loss: 0.6722 - val_accuracy: 0.9294\n",
      "Epoch 190/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5719 - accuracy: 0.9671 - val_loss: 0.6802 - val_accuracy: 0.9265\n",
      "Epoch 191/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5720 - accuracy: 0.9663 - val_loss: 0.6712 - val_accuracy: 0.9291\n",
      "Epoch 192/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5711 - accuracy: 0.9668 - val_loss: 0.6708 - val_accuracy: 0.9288\n",
      "Epoch 193/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5704 - accuracy: 0.9668 - val_loss: 0.6694 - val_accuracy: 0.9317\n",
      "Epoch 194/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5700 - accuracy: 0.9667 - val_loss: 0.6685 - val_accuracy: 0.9299\n",
      "Epoch 195/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5696 - accuracy: 0.9675 - val_loss: 0.6681 - val_accuracy: 0.9315\n",
      "Epoch 196/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5687 - accuracy: 0.9672 - val_loss: 0.6684 - val_accuracy: 0.9314\n",
      "Epoch 197/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5695 - accuracy: 0.9670 - val_loss: 0.6687 - val_accuracy: 0.9299\n",
      "Epoch 198/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5687 - accuracy: 0.9671 - val_loss: 0.6656 - val_accuracy: 0.9316\n",
      "Epoch 199/300\n",
      "611/611 [==============================] - 5s 7ms/step - loss: 0.5678 - accuracy: 0.9668 - val_loss: 0.6666 - val_accuracy: 0.9316\n",
      "Epoch 200/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5681 - accuracy: 0.9673 - val_loss: 0.6670 - val_accuracy: 0.9300\n",
      "Epoch 201/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5674 - accuracy: 0.9670 - val_loss: 0.6649 - val_accuracy: 0.9314\n",
      "Epoch 202/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5667 - accuracy: 0.9675 - val_loss: 0.6672 - val_accuracy: 0.9296\n",
      "Epoch 203/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5658 - accuracy: 0.9673 - val_loss: 0.6679 - val_accuracy: 0.9299\n",
      "Epoch 204/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5657 - accuracy: 0.9673 - val_loss: 0.6635 - val_accuracy: 0.9308\n",
      "Epoch 205/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5653 - accuracy: 0.9671 - val_loss: 0.6618 - val_accuracy: 0.9327\n",
      "Epoch 206/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5649 - accuracy: 0.9679 - val_loss: 0.6724 - val_accuracy: 0.9282\n",
      "Epoch 207/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5656 - accuracy: 0.9678 - val_loss: 0.6683 - val_accuracy: 0.9303\n",
      "Epoch 208/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5639 - accuracy: 0.9676 - val_loss: 0.6645 - val_accuracy: 0.9284\n",
      "Epoch 209/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5630 - accuracy: 0.9682 - val_loss: 0.6667 - val_accuracy: 0.9304\n",
      "Epoch 210/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5630 - accuracy: 0.9681 - val_loss: 0.6631 - val_accuracy: 0.9316\n",
      "Epoch 211/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5628 - accuracy: 0.9685 - val_loss: 0.6606 - val_accuracy: 0.9319\n",
      "Epoch 212/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5625 - accuracy: 0.9680 - val_loss: 0.6632 - val_accuracy: 0.9311\n",
      "Epoch 213/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5609 - accuracy: 0.9689 - val_loss: 0.6642 - val_accuracy: 0.9310\n",
      "Epoch 214/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5607 - accuracy: 0.9693 - val_loss: 0.6643 - val_accuracy: 0.9307\n",
      "Epoch 215/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5612 - accuracy: 0.9679 - val_loss: 0.6647 - val_accuracy: 0.9308\n",
      "Epoch 216/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5618 - accuracy: 0.9681 - val_loss: 0.6591 - val_accuracy: 0.9330\n",
      "Epoch 217/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5602 - accuracy: 0.9690 - val_loss: 0.6606 - val_accuracy: 0.9318\n",
      "Epoch 218/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5596 - accuracy: 0.9688 - val_loss: 0.6631 - val_accuracy: 0.9304\n",
      "Epoch 219/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5590 - accuracy: 0.9690 - val_loss: 0.6645 - val_accuracy: 0.9304\n",
      "Epoch 220/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5588 - accuracy: 0.9696 - val_loss: 0.6587 - val_accuracy: 0.9326\n",
      "Epoch 221/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5591 - accuracy: 0.9690 - val_loss: 0.6549 - val_accuracy: 0.9323\n",
      "Epoch 222/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5587 - accuracy: 0.9688 - val_loss: 0.6592 - val_accuracy: 0.9321\n",
      "Epoch 223/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5579 - accuracy: 0.9695 - val_loss: 0.6623 - val_accuracy: 0.9306\n",
      "Epoch 224/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5580 - accuracy: 0.9684 - val_loss: 0.6613 - val_accuracy: 0.9312\n",
      "Epoch 225/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5574 - accuracy: 0.9691 - val_loss: 0.6549 - val_accuracy: 0.9336\n",
      "Epoch 226/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5571 - accuracy: 0.9691 - val_loss: 0.6578 - val_accuracy: 0.9322\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5570 - accuracy: 0.9688 - val_loss: 0.6614 - val_accuracy: 0.9300\n",
      "Epoch 228/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5566 - accuracy: 0.9692 - val_loss: 0.6535 - val_accuracy: 0.9337\n",
      "Epoch 229/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5556 - accuracy: 0.9693 - val_loss: 0.6634 - val_accuracy: 0.9307\n",
      "Epoch 230/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5551 - accuracy: 0.9697 - val_loss: 0.6601 - val_accuracy: 0.9314\n",
      "Epoch 231/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5553 - accuracy: 0.9702 - val_loss: 0.6546 - val_accuracy: 0.9335\n",
      "Epoch 232/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5545 - accuracy: 0.9696 - val_loss: 0.6548 - val_accuracy: 0.9335\n",
      "Epoch 233/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5545 - accuracy: 0.9698 - val_loss: 0.6601 - val_accuracy: 0.9300\n",
      "Epoch 234/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5551 - accuracy: 0.9702 - val_loss: 0.6594 - val_accuracy: 0.9306\n",
      "Epoch 235/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5543 - accuracy: 0.9696 - val_loss: 0.6584 - val_accuracy: 0.9324\n",
      "Epoch 236/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5535 - accuracy: 0.9699 - val_loss: 0.6608 - val_accuracy: 0.9296\n",
      "Epoch 237/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5537 - accuracy: 0.9696 - val_loss: 0.6566 - val_accuracy: 0.9324\n",
      "Epoch 238/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5531 - accuracy: 0.9700 - val_loss: 0.6600 - val_accuracy: 0.9306\n",
      "Epoch 239/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5529 - accuracy: 0.9703 - val_loss: 0.6568 - val_accuracy: 0.9312\n",
      "Epoch 240/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5520 - accuracy: 0.9704 - val_loss: 0.6576 - val_accuracy: 0.9317\n",
      "Epoch 241/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5523 - accuracy: 0.9704 - val_loss: 0.6543 - val_accuracy: 0.9319\n",
      "Epoch 242/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5524 - accuracy: 0.9698 - val_loss: 0.6549 - val_accuracy: 0.9320\n",
      "Epoch 243/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5519 - accuracy: 0.9703 - val_loss: 0.6551 - val_accuracy: 0.9322\n",
      "Epoch 244/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5513 - accuracy: 0.9704 - val_loss: 0.6569 - val_accuracy: 0.9321\n",
      "Epoch 245/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5511 - accuracy: 0.9709 - val_loss: 0.6572 - val_accuracy: 0.9309\n",
      "Epoch 246/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5504 - accuracy: 0.9703 - val_loss: 0.6597 - val_accuracy: 0.9314\n",
      "Epoch 247/300\n",
      "611/611 [==============================] - 6s 9ms/step - loss: 0.5506 - accuracy: 0.9704 - val_loss: 0.6515 - val_accuracy: 0.9320\n",
      "Epoch 248/300\n",
      "611/611 [==============================] - 5s 9ms/step - loss: 0.5500 - accuracy: 0.9710 - val_loss: 0.6579 - val_accuracy: 0.9306\n",
      "Epoch 249/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5501 - accuracy: 0.9705 - val_loss: 0.6550 - val_accuracy: 0.9312\n",
      "Epoch 250/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5494 - accuracy: 0.9709 - val_loss: 0.6569 - val_accuracy: 0.9301\n",
      "Epoch 251/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5492 - accuracy: 0.9709 - val_loss: 0.6607 - val_accuracy: 0.9294\n",
      "Epoch 252/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5495 - accuracy: 0.9701 - val_loss: 0.6529 - val_accuracy: 0.9331\n",
      "Epoch 253/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5482 - accuracy: 0.9705 - val_loss: 0.6515 - val_accuracy: 0.9336\n",
      "Epoch 254/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5478 - accuracy: 0.9713 - val_loss: 0.6552 - val_accuracy: 0.9321\n",
      "Epoch 255/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5478 - accuracy: 0.9712 - val_loss: 0.6562 - val_accuracy: 0.9312\n",
      "Epoch 256/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5479 - accuracy: 0.9713 - val_loss: 0.6637 - val_accuracy: 0.9275\n",
      "Epoch 257/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5478 - accuracy: 0.9707 - val_loss: 0.6585 - val_accuracy: 0.9292\n",
      "Epoch 258/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5475 - accuracy: 0.9710 - val_loss: 0.6489 - val_accuracy: 0.9343\n",
      "Epoch 259/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5470 - accuracy: 0.9711 - val_loss: 0.6508 - val_accuracy: 0.9336\n",
      "Epoch 260/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5471 - accuracy: 0.9710 - val_loss: 0.6527 - val_accuracy: 0.9330\n",
      "Epoch 261/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5464 - accuracy: 0.9717 - val_loss: 0.6513 - val_accuracy: 0.9338\n",
      "Epoch 262/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5459 - accuracy: 0.9712 - val_loss: 0.6508 - val_accuracy: 0.9330\n",
      "Epoch 263/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5457 - accuracy: 0.9717 - val_loss: 0.6497 - val_accuracy: 0.9327\n",
      "Epoch 264/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5456 - accuracy: 0.9718 - val_loss: 0.6497 - val_accuracy: 0.9345\n",
      "Epoch 265/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5452 - accuracy: 0.9716 - val_loss: 0.6569 - val_accuracy: 0.9297\n",
      "Epoch 266/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5453 - accuracy: 0.9718 - val_loss: 0.6562 - val_accuracy: 0.9320\n",
      "Epoch 267/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5459 - accuracy: 0.9707 - val_loss: 0.6513 - val_accuracy: 0.9322\n",
      "Epoch 268/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5447 - accuracy: 0.9718 - val_loss: 0.6472 - val_accuracy: 0.9351\n",
      "Epoch 269/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5449 - accuracy: 0.9716 - val_loss: 0.6587 - val_accuracy: 0.9290\n",
      "Epoch 270/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5442 - accuracy: 0.9717 - val_loss: 0.6492 - val_accuracy: 0.9337\n",
      "Epoch 271/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5441 - accuracy: 0.9711 - val_loss: 0.6531 - val_accuracy: 0.9302\n",
      "Epoch 272/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5436 - accuracy: 0.9720 - val_loss: 0.6554 - val_accuracy: 0.9309\n",
      "Epoch 273/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5441 - accuracy: 0.9716 - val_loss: 0.6560 - val_accuracy: 0.9294\n",
      "Epoch 274/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5433 - accuracy: 0.9719 - val_loss: 0.6467 - val_accuracy: 0.9333\n",
      "Epoch 275/300\n",
      "611/611 [==============================] - 5s 8ms/step - loss: 0.5432 - accuracy: 0.9716 - val_loss: 0.6475 - val_accuracy: 0.9342\n",
      "Epoch 276/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5421 - accuracy: 0.9724 - val_loss: 0.6481 - val_accuracy: 0.9308\n",
      "Epoch 277/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5426 - accuracy: 0.9720 - val_loss: 0.6487 - val_accuracy: 0.9336\n",
      "Epoch 278/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5427 - accuracy: 0.9721 - val_loss: 0.6432 - val_accuracy: 0.9351\n",
      "Epoch 279/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5420 - accuracy: 0.9716 - val_loss: 0.6494 - val_accuracy: 0.9317\n",
      "Epoch 280/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5421 - accuracy: 0.9721 - val_loss: 0.6452 - val_accuracy: 0.9341\n",
      "Epoch 281/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5424 - accuracy: 0.9719 - val_loss: 0.6443 - val_accuracy: 0.9328\n",
      "Epoch 282/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5419 - accuracy: 0.9724 - val_loss: 0.6473 - val_accuracy: 0.9333\n",
      "Epoch 283/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5412 - accuracy: 0.9726 - val_loss: 0.6531 - val_accuracy: 0.9322\n",
      "Epoch 284/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5410 - accuracy: 0.9725 - val_loss: 0.6508 - val_accuracy: 0.9322\n",
      "Epoch 285/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5408 - accuracy: 0.9725 - val_loss: 0.6581 - val_accuracy: 0.9295\n",
      "Epoch 286/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5411 - accuracy: 0.9723 - val_loss: 0.6474 - val_accuracy: 0.9336\n",
      "Epoch 287/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5403 - accuracy: 0.9719 - val_loss: 0.6533 - val_accuracy: 0.9323\n",
      "Epoch 288/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5404 - accuracy: 0.9724 - val_loss: 0.6554 - val_accuracy: 0.9302\n",
      "Epoch 289/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5400 - accuracy: 0.9726 - val_loss: 0.6519 - val_accuracy: 0.9328\n",
      "Epoch 290/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5394 - accuracy: 0.9724 - val_loss: 0.6447 - val_accuracy: 0.9336\n",
      "Epoch 291/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5400 - accuracy: 0.9730 - val_loss: 0.6516 - val_accuracy: 0.9308\n",
      "Epoch 292/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5403 - accuracy: 0.9723 - val_loss: 0.6471 - val_accuracy: 0.9349\n",
      "Epoch 293/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5394 - accuracy: 0.9725 - val_loss: 0.6447 - val_accuracy: 0.9344\n",
      "Epoch 294/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5384 - accuracy: 0.9736 - val_loss: 0.6476 - val_accuracy: 0.9325\n",
      "Epoch 295/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5390 - accuracy: 0.9728 - val_loss: 0.6449 - val_accuracy: 0.9336\n",
      "Epoch 296/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5386 - accuracy: 0.9728 - val_loss: 0.6502 - val_accuracy: 0.9317\n",
      "Epoch 297/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5396 - accuracy: 0.9726 - val_loss: 0.6491 - val_accuracy: 0.9322\n",
      "Epoch 298/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5383 - accuracy: 0.9732 - val_loss: 0.6490 - val_accuracy: 0.9316\n",
      "Epoch 299/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5391 - accuracy: 0.9728 - val_loss: 0.6474 - val_accuracy: 0.9323\n",
      "Epoch 300/300\n",
      "611/611 [==============================] - 4s 7ms/step - loss: 0.5376 - accuracy: 0.9734 - val_loss: 0.6476 - val_accuracy: 0.9317\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.layers import Activation\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=1024,  kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
    "                                      bias_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(Dense(128,  kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
    "                                      bias_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "model.add(Dense(64,   kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
    "                                      bias_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Activation(activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(46,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                                      bias_regularizer=regularizers.l2(1e-4)))\n",
    "\n",
    "model.add(Activation(activation='softmax'))\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.00011)\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "History=model.fit(Train_X,Train_Y,validation_data=(Test_X, Test_Y), epochs=300, batch_size=128,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(History.history['accuracy'])\n",
    "plt.plot(History.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
